# Norm (mathematics)

## Concept

In mathematics, a norm is a function from a real or complex vector space to the non-negative real numbers that **behaves in certain ways like the distance from the origin**: it commutes with scaling, obeys a form of the triangle inequality, and is zero only at the origin.

范数（norm）最直观的理解就是到原点的距离：
- 比如一个实数轴上的点到0的距离$\left | x \right |$），这个很好理解
- 也可以是一个向量到零点的距离，比如平面坐标系里，$(3, 4)$到原点的欧式距离是5，这个计算的也是范式
- 然后扩展到一个矩阵，也可以求范式，只不过定义更加复杂了

### 为什么要引入范数？

我们都知道，函数与几何图形往往是有对应的关系，这个很好想象，特别是在三维以下的空间内，函数是几何图像的数学概括，而几何图像是函数的高度形象化。但当函数与几何超出三维空间时，就难以获得较好的想象，于是就有了映射的概念，进而引入范数的概念。

当我们有了范数的概念后，我们就可以引出两个向量的距离的定义，这个向量可以是任意维数的。通过距离的定义，进而我们可以讨论逼近程度，从而讨论收敛性、求极限。

计算机领域用的比较多的就是迭代过程中收敛性质的判断，一般迭代前后步骤的差值的范数表示其大小，常用的是二范数，差值越小表示越逼近实际值，可以认为达到要求的精度，收敛。

总的来说，范数存在的意义是为了实现比较距离。比如，在一维实数集合中，我们随便取两个点4和9，我们知道9比4大，但是到了二维实数空间中，取两个点（1，0）和（3，4），这个时候我们就没办法比较它们之间的大小，因为它们不是可以比较的实数，于是我们引入范数这个概念，把我们的（1，0）和（3，4）通过范数分别映射到实数1和5，这样我们就可以比较这两个点了。所以你可以看到，范数其实是一个函数，它把不能比较的向量转换成可以比较的实数。

## Definition


### 向量的范数

在数学上，对于向量范数的定义，就是只要满足以下三条性质的函数，我们就可以称为它为范数。

1. $\|x\| \geq 0$ 且 $\|x\| = 0 \iff x = 0$ 对 $x \in X$（正定性）
2. $\|cx\| = |c|\|x\|$ 对于 $c \in \mathbb{R}, x \in X$（齐次性）
3. $\|a + b\| \leq \|a\| + \|b\|$ 对任意 $a, b \in X$（三角不等式）

#### 常用的向量范数

##### 向量的0-范数

$$\|x\|_0 = \sum_{i=1}^k |x_i|^0$$

**意义**：非0元素个数

**评价**：L0范数表示向量中非零元素的个数。L0范数的这个属性，使其非常适用于机器学习中的稀疏编码。在特征选择中，通过最小化L0范数来寻找最少最优的稀疏特征项。但是，L0范数的最小化问题是NP难问题。而L1范数是L0范数的最优凸近似，它比L0范数要更容易求解。因此，优化过程将会被转换为更高维的范数（例如L1范数）问题。

##### 向量的1-范数

$$\|X\|_1 = \sum_{i=1}^n{|x_i|}$$

**意义**：各个元素的绝对值之和

##### 向量的2-范数 ( $l_2$-norm)

$$\|X\|_2 = (\sum_{i=1}^n{x_i^2})^{\frac{1}{2}}$$

**意义**：每个元素平方和再平方根

**评价**：L2范数是最常用的范数。我们用的最多的度量距离欧氏距离就是一种L2范数。在回归里面，有人把加了L2范数项的回归称为“岭回归”（Ridge Regression），有人也叫它“权值衰减(weight decay)”。它被广泛的应用在解决机器学习里的过拟合问题。

##### 向量的p-范数

$$\|X\|_p = (\sum_{i=1}^n{|x_i|^p})^\frac{1}{p}$$

**意义**：每个元素p次方和再p次方根

##### 向量的无穷范数

$$\|X\|_\infty = \max_{1\le i\le n}|x_i|$$

**意义**：向量的p-范数的p取无限大即为向量的无穷范数

### 矩阵的范数

跟向量的范数定义类似，只不过矩阵的范数的性质比向量的范数性质多了一条相容性。

1. $\|x\| \geq 0$ 且 $\|x\| = 0 \iff x = 0$ 对 $x \in X$（正定性）
2. $\|cx\| = |c|\|x\|$ 对于 $c \in \mathbb{R}, x \in X$（齐次性）
3. $\|a + b\| \leq \|a\| + \|b\|$ 对任意 $a, b \in X$（三角不等式）
4. 矩阵的相容性：对于 $n \times n$ 矩阵 $A$ 和 $B$，有 $\|AB\| \leq \|A\| \cdot \|B\|$

#### 常用的矩阵范数

##### 矩阵的1-范数（列模）

$\|A\|_1 = \max_j\sum_{i=1}^m|a_{i,j}|$

**意义**：矩阵的列向量的和的最大值

##### 矩阵的2-范数（谱模）

$\|A\|_2 = \sqrt{\lambda_{\max}(A^TA)} = \sqrt{\lambda_1}$

**意义**：$A^TA$ 矩阵的最大特征值的平方根

##### 矩阵的 $\infty$-范数（行模）

$\|A\|_\infty = \max_i\sum_{j=1}^i|a_{i,j}|$

**意义**：矩阵的行向量的和的最大值

##### 矩阵的F-范数

$\|A\|_F = (\sum_{i=1}^m\sum_{j=1}^n|a_{i,j}|^2)^{\frac{1}{2}}$

也可以描述为：

$\|A\|_F = \sqrt{Tr(AA^T)}$

**意义**：Frobenius范数，即矩阵元素绝对值的平方和再开平方。

## 参考文献

1. [带你秒懂向量与矩阵的范数(Norm)](https://blog.csdn.net/weixin_43660703/article/details/108422077)
2. [参考文献1](https://blog.csdn.net/weixin_43660703/article/details/108422077)
3. [参考文献2](https://blog.csdn.net/weixin_43660703/article/details/108422077)

