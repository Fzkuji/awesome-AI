# Gradient descent

梯度下降是一种优化算法，主要用于机器学习和深度学习中的参数优化。在你提供的线性模型$y_{pred} = wx + b$中，我们的目标是通过最小化损失函数来找到最优的参数$w$和$b$。

首先，我们需要定义一个损失函数，这个函数用于衡量我们模型的预测值$y_{pred}$与真实值$y$之间的差距。在回归问题中，常用的损失函数是均方误差（Mean Squared Error，MSE），其公式为：

$$L = \frac{1}{n}\sum_{i=1}^{n}(y_{pred}^{(i)} - y^{(i)})^2$$

其中，$n$是样本数量，$y_{pred}^{(i)}$和$y^{(i)}$分别是第$i$个样本的预测值和真实值。

我们的目标是找到一组参数$w$和$b$，使得损失函数$L$最小。这就是一个优化问题，我们可以通过梯度下降法来求解。

梯度下降法的基本思想是：在每一步，我们计算损失函数$L$关于参数$w$和$b$的梯度，然后沿着梯度的反方向更新参数。这样做的原因是，梯度指向的方向是函数值增加最快的方向，所以我们沿着梯度的反方向走，就可以使函数值减小最快。

损失函数$L$关于参数$w$和$b$的梯度分别为：

$$\frac{\partial L}{\partial w} = \frac{2}{n}\sum_{i=1}^{n}x^{(i)}(wx^{(i)} + b - y^{(i)})$$

$$\frac{\partial L}{\partial b} = \frac{2}{n}\sum_{i=1}^{n}(wx^{(i)} + b - y^{(i)})$$

然后，我们可以用以下公式来更新参数$w$和$b$：

$$w = w - \alpha \frac{\partial L}{\partial w}$$

$$b = b - \alpha \frac{\partial L}{\partial b}$$

其中，$\alpha$是学习率，是一个超参数，需要我们手动设置。学习率决定了我们在梯度方向上走的步长。如果学习率设置得过大，可能会导致优化过程震荡不收敛；如果设置得过小，优化过程可能会非常慢。
