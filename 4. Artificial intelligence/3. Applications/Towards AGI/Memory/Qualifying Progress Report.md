具体见Overleaf

Recurrent Neural Networks (RNNs), including models like Long Short-Term Memory (LSTM) networks, have historically faced challenges in capturing long-range dependencies in sequential data. This issue primarily arises from the fixed size of the hidden states, which will lose previous information as the sequence grows. To address this, the introduction of Transformer architectures marked a significant advancement, as they allow for modeling long-range dependencies without relying on recurrence. However, Transformers process entire sequences in parallel, resulting in computational complexity that scales quadratically with the sequence length. This quadratic scaling presents a major computational bottleneck, especially for tasks involving very long sequences.



\subsection{Overview} The existing LLMs are computationally intensive with deep architectures, leading to inefficiencies that limit their application in scenarios requiring rapid response. Various studies have attempted to address these challenges. For example, some research explores early-exit strategies to reduce the number of layers used during inference, significantly decreasing computation while maintaining performance【1】. Other studies propose the reduction of attention heads and feed-forward networks in large models to improve efficiency without sacrificing in-context learning capabilities【2】. These efforts highlight the need for further advancements in LLMs to make them more practical for real-world applications. 


\subsection{Background} The foundational architecture of LLMs, particularly the Transformer model, has been a focal point in NLP research. The Transformer’s self-attention mechanism has undergone various optimizations, such as the exploration of bidirectional versus unidirectional masking and early-exit mechanisms【3】【4】. Additionally, research has suggested that not all layers are necessary during inference, proposing more efficient use of computational resources【5】. Moreover, alternative architectures like the Recurrent Transformer introduce memory-like components that adapt over time, enhancing performance for specific tasks【6】【7】. Memory-augmented models have also been proposed, which store and retrieve information from previous computations, thereby improving the model's ability to handle longer sequences【8】【9】. 


\subsection{Relevance/Impact} The development of LLMs with implicit memory mechanisms is crucial as it addresses the inefficiencies present in current models. By integrating implicit memory, LLMs can reduce computational overhead, making them more suitable for real-time applications such as conversational agents and autonomous systems. This research not only has the potential to advance the scientific understanding of LLM architectures but also offers tangible benefits to society. For instance, more efficient models could lead to better resource utilization in data centers, reducing environmental impact. Additionally, these advancements may enhance the accessibility of AI technologies in various sectors, contributing to economic growth and improving the quality of life by enabling faster and more reliable AI-driven services.