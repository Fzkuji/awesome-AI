### 神经语言模型的扩展规律

#### 作者：
- Jared Kaplan（约翰霍普金斯大学，OpenAI）
- Sam McCandlish（OpenAI）
- Tom Henighan（OpenAI）
- Tom B. Brown（OpenAI）
- Benjamin Chess（OpenAI）
- Rewon Child（OpenAI）
- Scott Gray（OpenAI）
- Alec Radford（OpenAI）
- Jeffrey Wu（OpenAI）
- Dario Amodei（OpenAI）

#### 摘要：
本论文研究了语言模型在交叉熵损失上的经验扩展规律。损失随着模型大小、数据集大小和训练计算量的增加呈幂律缩放，某些趋势跨越了七个数量级。网络宽度或深度等其他架构细节在很大范围内影响较小。简单的公式描述了模型/数据集大小对过拟合的依赖性以及训练速度对模型大小的依赖性。这些关系使我们能够确定固定计算预算的最佳分配。较大的模型在样本效率上显著更高，最优计算效率的训练涉及在相对适中的数据量上训练非常大的模型，并在显著收敛前停止。

#### 关键点：
1. **语言模型性能与扩展**：
   - 模型性能主要依赖于模型参数数量（N）、数据集大小（D）和训练所用计算量（C）。
   - 模型的其他架构参数（如深度和宽度）在较大范围内对性能的影响较小。

2. **幂律规律**：
   - 性能与扩展因子N、D、C的关系呈幂律关系，这些趋势跨越了六个数量级。
   - 没有迹象表明这些趋势在上限偏离，但性能最终必须在达到零损失之前平稳。

3. **过拟合的普遍性**：
   - 只要我们按比例增加N和D，性能就会稳定提高，但如果固定N或D而增加另一个，则性能回报会逐渐减少。
   - 每次将模型大小增加8倍，只需将数据量增加约5倍以避免性能损失。

4. **训练的普遍性**：
   - 训练曲线遵循可预测的幂律，这些幂律参数大致独立于模型大小。
   - 可以通过外推训练曲线的早期部分来粗略预测如果训练更久将达到的损失。

5. **样本效率和收敛效率**：
   - 大模型在达到相同性能水平时所需的优化步骤较少，使用的数据点也较少。
   - 在固定计算预算下，最优性能通过训练非常大的模型并显著提前停止来实现，而不是训练小模型直至收敛。

6. **批量大小的影响**：
   - 理想的批量大小仅与损失的幂有关，并可以通过测量梯度噪声规模确定。

#### 实践意义：
- **研究人员**：提供了语言模型性能随着模型大小、数据集大小和计算量的可预测规律，为优化语言模型的设计和训练提供了理论基础。
- **开发人员**：展示了如何在计算资源有限的情况下，通过适当的扩展模型大小和数据集来实现最佳性能。

详情请参阅完整论文 [链接](https://arxiv.org/pdf/2001.08361)。