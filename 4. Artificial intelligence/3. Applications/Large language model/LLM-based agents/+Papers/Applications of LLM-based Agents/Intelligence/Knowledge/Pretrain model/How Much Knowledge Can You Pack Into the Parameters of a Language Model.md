### 论文摘要：在语言模型的参数中能容纳多少知识？

#### 作者：
- Adam Roberts（谷歌）
- Colin Raffel（谷歌）
- Noam Shazeer（谷歌）

#### 摘要：
近期研究表明，通过训练在非结构化文本上的神经语言模型，可以隐式存储和检索知识。本文通过微调预训练模型来回答问题，而不访问任何外部上下文或知识，测量了这种方法的实用性。结果表明，模型规模越大，性能越好，且其表现与明确从外部知识源检索答案的开放域系统相当。为了促进再现性和未来研究，作者公开了代码和训练模型。

#### 关键点：
1. **研究背景**：
   - 语言模型在无监督目标上进行预训练，然后在下游任务上进行微调，已被证明非常有效。
   - 预训练的大型深度神经语言模型可以隐式形成一个“知识库”，这些模型在自然语言理解任务中表现出色。

2. **研究方法**：
   - **实验模型**：使用谷歌的T5（Text-to-Text Transfer Transformer）模型。
   - **任务**：开放域问答（Open-domain Question Answering），在不提供外部知识的情况下回答问题。
   - **模型规模**：从220百万参数到11亿参数的不同模型规模。

3. **实验结果**：
   - 随着模型规模的增加，回答问题的性能不断提高。
   - 使用“显著跨度掩码”（Salient Span Masking，SSM）预训练目标进一步提高了性能。
   - 最大的模型（T5-11B和T5.1.1-XXL）在WebQuestions和TriviaQA数据集上达到了最好的效果。

4. **讨论**：
   - **知识存储**：大型语言模型可以在不访问外部知识源的情况下存储和检索大量知识。
   - **计算成本**：虽然大型模型计算成本高，但它们避免了传统开放域问答系统中昂贵的知识查找和长文档处理步骤。
   - **模型解释性**：当前的模型将知识分布在参数中，缺乏明确的解释机制，未来需要探索更具解释性的模型。
   - **任务多样性**：当前研究主要集中在“问答”任务上，未来需要评估在需要推理能力的任务上的表现。

#### 实践意义：
- **研究人员**：该研究提供了一种新的设计问答系统的方法，即通过预训练大规模语言模型来内化知识。
- **开发人员**：揭示了在资源受限的环境中，如何有效地利用大型预训练模型进行问答任务。

详情请参阅完整论文 [链接](https://arxiv.org/pdf/2002.08910)。