Title: Recommender Systems with Generative Retrieval

- Transformer Index for GEnerative Recommenders (TIGER)
- Affiliation: University of Wisconsin-Madison, Google
- Keywords: [[Recommender system|Recommender Systems]], Generative Retrieval, Vector Quantization

- https://shashankrajput.github.io/Generative.pdf
- https://arxiv.org/abs/2305.05065
- **No code**

2018å¹´çš„æ–‡ç« ï¼Œæ–‡æ¡£å¹¶æœªå‘è¡¨åœ¨åˆŠç‰©ä¸Šï¼Œè€Œæ˜¯ç›´æ¥å­˜åœ¨äºä½œè€…çš„GitHubä¸Šï¼Œå¯èƒ½æ˜¯æ²¡å‘å‡ºå»å§
å‘ç°åˆè¢«ä¼ åˆ°arxiväº†ï¼ˆ8 May 2023ï¼‰

æ€»ç»“ï¼šå°±æ˜¯è®¾è®¡äº†ä¸€ç§embeddingæ–¹æ³•ï¼Œå°†æ‰€æœ‰ç‰©å“è¡¨ç¤ºä¸ºä¸€äº›æ•´æ•°çš„ç»„åˆã€‚å®éªŒè¯æ˜è¿™æ¯”å°æ•°çº§åˆ«çš„åµŒå…¥æ›´å¥½ã€‚ç„¶åæ¨¡å‹å°è¯•é¢„æµ‹è¿™ä¸ªç‚¹å‡»åºåˆ—ã€‚

## Abstract

ç°ä»£æ¨èç³»ç»Ÿåˆ©ç”¨å¤§è§„æ¨¡æ£€ç´¢æ¨¡å‹ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µï¼šé¦–å…ˆè®­ç»ƒåŒç¼–ç å™¨æ¨¡å‹å°†æŸ¥è¯¢å’Œå€™é€‰é¡¹åµŒå…¥åˆ°åŒä¸€ç©ºé—´ï¼Œç„¶åä½¿ç”¨è¿‘ä¼¼æœ€è¿‘é‚»ï¼ˆANNï¼‰æœç´¢æ ¹æ®æŸ¥è¯¢çš„åµŒå…¥æ¥é€‰æ‹©æœ€ä½³å€™é€‰é¡¹ã€‚

åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„å•é˜¶æ®µèŒƒå¼ï¼šä¸€ç§ç”Ÿæˆå¼æ£€ç´¢æ¨¡å‹ï¼Œå®ƒä»¥è‡ªå›å½’çš„æ–¹å¼è§£ç ç›®æ ‡å€™é€‰é¡¹çš„æ ‡è¯†ç¬¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä¸æ˜¯ä¸ºæ¯ä¸ªé¡¹ç›®åˆ†é…éšæœºç”Ÿæˆçš„åŸå­IDï¼Œè€Œæ˜¯ç”Ÿæˆè¯­ä¹‰IDï¼šä¸ºæ¯ä¸ªé¡¹ç›®ç”Ÿæˆä¸€ä¸ªå…·æœ‰è¯­ä¹‰æ„ä¹‰çš„ç¼–ç è¯å…ƒç»„ä½œä¸ºå…¶å”¯ä¸€æ ‡è¯†ç¬¦ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ç§åä¸ºRQ-VAEçš„åˆ†å±‚æ–¹æ³•ç”Ÿæˆè¿™äº›ç¼–ç è¯ã€‚ä¸€æ—¦æˆ‘ä»¬æ‹¥æœ‰æ‰€æœ‰é¡¹ç›®çš„è¯­ä¹‰IDï¼Œå°±å¯ä»¥è®­ç»ƒä¸€ä¸ªåŸºäºTransformerçš„åºåˆ—åˆ°åºåˆ—æ¨¡å‹æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªé¡¹ç›®çš„è¯­ä¹‰IDã€‚ç”±äºè¯¥æ¨¡å‹ä»¥è‡ªå›å½’æ–¹å¼ç›´æ¥é¢„æµ‹è¯†åˆ«ä¸‹ä¸€ä¸ªé¡¹ç›®çš„ç¼–ç è¯å…ƒç»„ï¼Œå› æ­¤å¯ä»¥å°†å…¶è§†ä¸ºç”Ÿæˆå¼æ£€ç´¢æ¨¡å‹ã€‚

æˆ‘ä»¬è¯æ˜äº†åœ¨è¿™ä¸ªæ–°èŒƒå¼ä¸‹è®­ç»ƒçš„æ¨èç³»ç»Ÿæ”¹è¿›äº†å½“å‰åœ¨äºšé©¬é€Šæ•°æ®é›†ä¸Šçš„æœ€å…ˆè¿›æ¨¡å‹æ‰€å–å¾—çš„ç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†åºåˆ—åˆ°åºåˆ—æ¨¡å‹ä¸åˆ†å±‚è¯­ä¹‰IDç›¸ç»“åˆèƒ½å¤Ÿæ›´å¥½åœ°æ³›åŒ–ï¼Œä»è€Œæ”¹è¿›å¯¹å†·å¯åŠ¨é¡¹ç›®çš„æ£€ç´¢ä»¥æä¾›æ¨èã€‚

## Introduction

æ¨èç³»ç»Ÿå¸®åŠ©ç”¨æˆ·å‘ç°æ„Ÿå…´è¶£çš„å†…å®¹ï¼Œåœ¨è¯¸å¦‚è§†é¢‘ã€åº”ç”¨ã€äº§å“å’ŒéŸ³ä¹ç­‰é¢†åŸŸå¹¿æ³›åº”ç”¨ã€‚ç°ä»£æ¨èç³»ç»Ÿé‡‡ç”¨æ£€ç´¢å’Œæ’åºç­–ç•¥ï¼Œå…ˆåœ¨æ£€ç´¢é˜¶æ®µé€‰æ‹©ä¸€ç»„åˆé€‚çš„å€™é€‰é¡¹ï¼Œç„¶åä½¿ç”¨æ’åºæ¨¡å‹å¯¹å…¶è¿›è¡Œæ’åºã€‚

ä¸ºäº†æ›´å¥½åœ°æ•æ‰æ•°æ®ä¸­çš„éçº¿æ€§ï¼Œè¿‘å¹´æ¥åŒç¼–ç å™¨æ¶æ„ï¼ˆå³ä¸€ä¸ªç”¨äºæŸ¥è¯¢ï¼Œä¸€ä¸ªç”¨äºå€™é€‰é¡¹ï¼‰è¶Šæ¥è¶Šå—æ¬¢è¿ï¼Œå®ƒä½¿ç”¨å†…ç§¯å°†æŸ¥è¯¢å’Œå€™é€‰é¡¹åµŒå…¥åˆ°ç›¸åŒçš„ç©ºé—´ä¸­ã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨å€™é€‰å¡”ä¸ºæ‰€æœ‰é¡¹ç›®åˆ›å»ºåµŒå…¥ç´¢å¼•ã€‚å¯¹äºç»™å®šçš„æŸ¥è¯¢ï¼Œä½¿ç”¨æŸ¥è¯¢å¡”è®¡ç®—å…¶åµŒå…¥ï¼Œç„¶åä½¿ç”¨è¿‘ä¼¼æœ€è¿‘é‚»ç®—æ³•é€‰æ‹©æœ€è¿‘çš„å€™é€‰é¡¹ã€‚å¦ä¸€æ–¹é¢ï¼Œæœ€è¿‘æµè¡Œçš„é¡ºåºæ¨èå™¨ä¼šæ˜¾å¼è€ƒè™‘ç”¨æˆ·ä¸é¡¹ç›®äº¤äº’çš„é¡ºåºã€‚å®ƒä»¬é€šå¸¸åœ¨è¾“å‡ºå±‚ä½¿ç”¨softmaxï¼Œå¹¶åœ¨æ¨ç†æœŸé—´ä½¿ç”¨ ANNã€‚


![[Fig 1 1.png]]
Figure 1: High-level overview of Transformer Index for GEnerative Recommenders (TIGER) framework. TIGER proposes representing an item as a tuple of discrete semantic tokens (referred to as Semantic ID), which allows framing the sequential recommendation task as a generative task such that the Semantic ID of the next item is directly predicted using a sequence-to-sequence model. ^c9c8a3

æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºé¡ºåºæ¨èå™¨çš„ç”Ÿæˆå¼æ£€ç´¢æ¨¡å‹çš„æ–°èŒƒå¼ã€‚ä¸ä¼ ç»Ÿçš„æŸ¥è¯¢-å€™é€‰é¡¹åŒ¹é…æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä½¿ç”¨ç«¯åˆ°ç«¯çš„ç”Ÿæˆæ¨¡å‹ç›´æ¥é¢„æµ‹å€™é€‰é¡¹IDï¼Œä»è€Œæ— éœ€ç¦»æ•£çš„ã€éå¯å¾®çš„å†…ç§¯æœç´¢ç³»ç»Ÿæˆ–ç´¢å¼•ã€‚

é€šè¿‡autoregressive decodingå’Œ[[Beam search]]ï¼Œæˆ‘ä»¬å¯ä»¥æ£€ç´¢åˆ°å¤šä¸ªå¯è¡Œçš„å€™é€‰é¡¹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥å°†Transformerçš„å†…å­˜ï¼ˆå‚æ•°ï¼‰è§£é‡Šä¸ºend-to-end recommendation indexã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•ç§°ä¸ºTIGERï¼Œå³*Transformer Index for GEnerative Recommenders*ã€‚TIGERçš„æ¦‚è¿°å¦‚[[TIGER#^c9c8a3|Figure 1]]æ‰€ç¤ºã€‚

TIGERçš„ç‰¹ç‚¹æ˜¯ç”¨ä¸€ç§æ–°é¢–çš„â€œè¯­ä¹‰IDâ€è¡¨ç¤ºæ¯ä¸ªé¡¹ç›®ï¼šåŸºäºé¡¹ç›®å†…å®¹ä¿¡æ¯ï¼ˆå¦‚æ–‡æœ¬æè¿°ï¼‰çš„ä¸€ç³»åˆ—æ ‡è®°ã€‚

å…·ä½“æ¥è¯´ï¼Œç»™å®šä¸€ä¸ªé¡¹ç›®çš„æ–‡æœ¬æè¿°ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨é¢„è®­ç»ƒçš„æ–‡æœ¬ç¼–ç å™¨ï¼ˆä¾‹å¦‚SentenceT5ï¼‰æ¥ç”Ÿæˆå¯†é›†çš„å†…å®¹åµŒå…¥ã€‚ ç„¶åå¯ä»¥åœ¨åµŒå…¥ä¸Šåº”ç”¨é‡åŒ–æ–¹æ¡ˆä»¥å½¢æˆä¸€å°ç»„ä»¤ç‰Œ/ç å­—ï¼ˆæ•´æ•°ï¼‰ã€‚ æˆ‘ä»¬å°†è¿™ä¸ªæœ‰åºçš„ä»£ç å­—å…ƒç»„ç§°ä¸ºé¡¹ç›®çš„è¯­ä¹‰ IDã€‚ æˆ‘ä»¬ä»äººç±»è¯­è¨€ä¸­è·å¾—äº†è¿™ä¸ªæƒ³æ³•çš„çµæ„Ÿï¼Œåœ¨äººç±»è¯­è¨€ä¸­ï¼Œæˆ‘ä»¬ç”¨è¯æ¥è¡¨ç¤ºæ¦‚å¿µï¼Œå¹¶å°†è¯ä¸²åœ¨ä¸€èµ·æ¥ä¼ è¾¾å¤æ‚çš„æƒ³æ³•ã€‚ åŒæ ·ï¼Œæˆ‘ä»¬æƒ³ä¸º ID å¼€å‘ä¸€ç§è¯­è¨€æ¥è¡¨ç¤ºé¡¹ç›®ã€‚ é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸€ç»„æœ‰é™çš„å•è¯ï¼ˆæ ‡è®°ï¼‰åºåˆ—æ¥è¡¨ç¤ºæ•°åäº¿ä¸ªé¡¹ç›®ï¼Œè¿™ä¸éšæœºç”Ÿæˆçš„åŸå­ ID éœ€è¦æ¯ä¸ªé¡¹ç›®ä¸€ä¸ª ID å½¢æˆå¯¹æ¯”ã€‚

å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç±»ä¼¼çš„æƒ³æ³•å·²ç»è¢«é‡‡ç”¨åœ¨ä»æ–‡æœ¬ç”Ÿæˆå›¾åƒçš„åœºæ™¯ä¸­ï¼Œæ¯”å¦‚[Parti](https://arxiv.org/abs/2206.10789)æ¨¡å‹ï¼Œå…¶ä½¿ç”¨ViT-VQGANå°†å›¾åƒè¡¨ç¤ºä¸ºæ ‡è®°ã€‚ä¸€ä¸ªå…³é”®çš„åŒºåˆ«æ˜¯ï¼Œåœ¨å›¾åƒç”Ÿæˆä¸­ï¼Œä¸€äº›é”™è¯¯çš„æ ‡è®°ä¼šå¯¼è‡´å›¾åƒä¸­çš„å°é”™è¯¯æˆ–å™ªå£°ã€‚ç›¸åï¼Œåœ¨è¿™é‡Œï¼Œä¸€ä¸ªé”™è¯¯çš„æ ‡è®°å°†æ„å‘³ç€æ¨èç³»ç»Ÿé¢„æµ‹åˆ°ä¸€ä¸ªä¸åŒçš„æˆ–ä¸å­˜åœ¨çš„é¡¹ç›®ã€‚

ä½¿ç”¨è¿™äº›è¯­ä¹‰IDæœ‰å¾ˆå¤šå¥½å¤„ã€‚ç°ä»£æ¨èç³»ç»Ÿä¸­çš„ç”¨æˆ·å’Œé¡¹ç›®æ•°é‡å¯èƒ½è¾¾åˆ°æ•°åäº¿ã€‚å› æ­¤ï¼Œå½“ä½¿ç”¨åŸå­IDå’ŒåµŒå…¥å‘é‡ä¹‹é—´çš„1:1æ˜ å°„æ—¶ï¼ŒåŸºäºANNçš„æ¨¡å‹çš„åµŒå…¥è¡¨å¯èƒ½å˜å¾—è¿‡å¤§ã€‚è¿™ä¸ä»…éœ€è¦å¤§é‡çš„å†…å­˜å’Œå­˜å‚¨ç©ºé—´ï¼Œè€Œä¸”åœ¨è®­ç»ƒé¡¹ç›®çš„åµŒå…¥æ—¶ä¹Ÿä¼šå¯¼è‡´ä¸å¹³è¡¡ï¼Œæµè¡Œçš„é¡¹ç›®ä¼šè¢«è¿‡åº¦é‡‡æ ·ï¼Œç›¸æ¯”ä¹‹ä¸‹ä¸å¸¸è§çš„é¡¹ç›®å°±ä¼šè¢«å¿½ç•¥ã€‚å› æ­¤ï¼Œé€šå¸¸çš„åšæ³•æ˜¯ä¸ºæ›´å—æ¬¢è¿çš„é¡¹ç›®ä¿ç•™ä¸“ç”¨è¯æ±‡è¡¨ï¼Œå¹¶å°†å…¶ä½™é¡¹ç›®éšæœºæ•£åˆ—åˆ°å›ºå®šæ•°é‡çš„æ¡¶ä¸­ã€‚ç„¶è€Œï¼Œæ•£åˆ—æ–¹æ¡ˆä¼šå¯¼è‡´éšæœºç¢°æ’ã€‚éšç€ç°å®ä¸–ç•Œæ¨èäº§å“ä¸­æ–°é¡¹ç›®çš„æ¨å‡ºï¼Œéšæœºç¢°æ’å¯èƒ½ä¼šåŠ å‰§å†·å¯åŠ¨é—®é¢˜ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åªéœ€è¦ç»´æŠ¤ä¸€ä¸ªå°å‹tokené›†çš„embedding tableã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„å†²çªåœ¨è¯­ä¹‰ä¸Šæ˜¯æœ‰æ„ä¹‰çš„ï¼Œè¿™æœ‰åŠ©äºç¼“è§£å†·å¯åŠ¨é—®é¢˜ã€‚

(1) æˆ‘ä»¬æå‡ºäº† TIGERï¼Œä¸€ç§æ–°é¢–çš„åŸºäºæ£€ç´¢çš„ç”Ÿæˆå¼æ¨èæ¨¡å‹ï¼Œå®ƒé¦–å…ˆä¸ºæ¯ä¸ªé¡¹ç›®åˆ†é…å”¯ä¸€çš„è¯­ä¹‰ IDï¼Œç„¶åè®­ç»ƒä¸€ä¸ªæ£€ç´¢æ¨¡å‹æ¥é¢„æµ‹ç”¨æˆ·å°†å‚ä¸çš„ä¸‹ä¸€ä¸ªé¡¹ç›®çš„è¯­ä¹‰ IDã€‚ è¿™ä¸ºåŸºäºé«˜ç»´æœ€è¿‘é‚»æœç´¢æˆ–åŸºäº softmax çš„æ¨èç³»ç»Ÿæä¾›äº†æ›¿ä»£æ–¹æ¡ˆã€‚ 
(2) æˆ‘ä»¬è¯æ˜ TIGER åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šä¼˜äºç°æœ‰ SOTA æ¨èç³»ç»Ÿçš„å¬å›ç‡å’Œ NDCG æŒ‡æ ‡ã€‚
(3) æˆ‘ä»¬å‘ç°è¿™ç§æ–°çš„ç”Ÿæˆæ£€ç´¢èŒƒå¼åœ¨é¡ºåºæ¨èç³»ç»Ÿä¸­å¸¦æ¥äº†ä¸¤ä¸ªé¢å¤–çš„èƒ½åŠ›ï¼š
1. èƒ½å¤Ÿæ¨èæ–°çš„å’Œä¸é¢‘ç¹çš„é¡¹ç›®ï¼Œä»è€Œæ”¹å–„å†·å¯åŠ¨é—®é¢˜
2. èƒ½å¤Ÿä½¿ç”¨ç”Ÿæˆå¤šæ ·åŒ–çš„æ¨èå¯è°ƒå‚æ•°ã€‚

## Methods

### Semantic ID generation

æœ¬è´¨å°±æ˜¯embeddingï¼Œç¾å…¶åæ›°semantic ID generationã€‚

![[Fig 2 (a).png]]

1. Use the textual description of items as content features and use Sentence-T5 encoder on this textual description.
2. The semantic embeddings are then quantized to generate a Semantic ID for each item.

#### RQ-VAE for Semantic IDs

Residual-Quantized Variational AutoEncoder (RQ-VAE) is a multi-stage vector quantizer that applies quantization on residuals at multiple levels to generate a tuple of codewords (aka Semantic IDs).

![[Fig 3 1.png]]
Figure 3: RQ-VAE: In the figure, the vector output by the DNN Encoder, say $r_0$ (represented by the blue bar), is fed to the quantizer, which works iteratively. First, the closest vector to $r_0$ is found in the first level codebook. Let this closest vector be $\boldsymbol{e}_{c_0}$ (represented by the red bar). Then, the residual error is computed as $r_1:=r_0-e_{c_0}$. This is fed into the second level of the quantizer, and the process is repeated: The closest vector to $r_1$ is found in the second level, say $e_{c_1}$ (represented by the green bar), and then the second level residual error is computed as $r_2=r_1-e_{c_1}^{\prime}$. Then, the process is repeated for a third time on $r_2$. The semantic codes are computed as the indices of $e_{c_0}$, $e_{c_1}$, and $e_{c_2}$ in their respective codebooks. In the example shown in the figure, this results in the code $(7,1,2)$.

#### Other options for quantization

A simple alternative to generating Semantic IDs is to use Locality Sensitive Hashing (LSH). We perform an ablation study where we find that RQ-VAE indeed works better than LSH. 

Another option is to use kmeans clustering hierarchically, but it loses semantic meaning between different clusters. 

We also tried VQ-VAE, and while it performs similarly to RQ-VAE for generating the candidates during retrieval, it loses the hierarchical nature of the IDs which confers many new capabilities.

#### Handling collisions

For retrieval, we would like to **avoid** collisions and assign unique IDs for items. 

But collisions may still occur. 

To remove the collisions we append an extra token at the end of the Semantic IDs to make them unique. For example, if two items share the Semantic ID (12, 24, 52), we append extra tokens to differentiate between them, hence they are represented as (12, 24, 52, 0) and (12, 24, 52, 1).

### Generative retrieval with Semantic IDs

ä¹Ÿå°±æ˜¯ç”¨ ç”¨æˆ·ä¹‹å‰ç‚¹å‡»çš„å•†å“æ¥é¢„æµ‹ç”¨æˆ·æ¥ä¸‹æ¥ä¼šç‚¹å‡»ä»€ä¹ˆã€‚ä»¥å‰çš„è®°å½•ä»¥ä¸Šé¢çš„Semantic IDçš„å½¢å¼è®°å½•ã€‚

We construct item sequences for every user by sorting chronologically the items they have interacted with. Then, given a sequence of the form (item ite $_1, \ldots$, item $m_n$ ), the recommender system's task is to predict the next item item ${ }_{n+1}$. For this, we propose a generative approach that directly predicts the Semantic IDs of items.

Formally, let $\left(c_{i, 0}, \ldots, c_{i, m-1}\right)$ be the $m$-length Semantic ID for item $_i$. Then, we convert the item sequence to the sequence $\left(c_{1,0}, \ldots, c_{1, m-1}, c_{2,0}, \ldots, c_{2, m-1}, c_{n, 0}, \ldots, c_{n, m-1}\right)$. The sequence-towhich is $\left(c_{n+1,0}, \ldots, c_{n+1, m-1}\right)$. Hence, this formulation does not need to make any major modifications to existing sequence-tosequence model architectures to train them for generative recommendations. Once we have the predicted tuple of codewords $\left(c_{n+1,0}, \ldots, c_{n+1, m-1}\right)$, we simply look up the item to which this Semantic ID corresponds to. There is a possibility that the generated Semantic ID does not match any item in the dataset. However, as we observe in Figure 6, the probability of such an event is very low.

## Evaluation

### Experimental setup

#### Datasets

In particular, we use three categories of the Amazon Product Reviews dataset for the sequential recommendation task: â€œBeautyâ€, â€œSports and Outdoorsâ€, and â€œToys and Gamesâ€.

Table 2: Dataset statistics for the three real-world benchmarks.

| Dataset | # Users | # Items | Sequence_ | Length   |
| :--- | :---: | :---: | :---: | :---: |
|  |  |  | Mean | Median |
| Beauty | 22,363 | 12,101 | 8.87 | 6 |
| Sports and Outdoors | 35,598 | 18,357 | 8.32 | 6 |
| Toys and Games | 19,412 | 11,924 | 8.63 | 6 |

#### Evaluation metrics.

We use

- top-k Recall (Recall@K)
- Normalized Discounted Cumulative Gain (NDCG@K) with ğ¾ = 5, 10 

to evaluate the recommendation performance.

#### Implementation details.

##### RQ-VAE model

We use the **pre-trained Sentence-T5 model** to obtain the semantic embedding of each item in the dataset.

In particular, we use itemâ€™s content features **such as title, price, brand, and category** to construct a sentence, which is then passed to the pre-trained Sentence-T5 model to obtain the itemâ€™s semantic embedding of **768 dimension**.

æ­¤å¤„çœç•¥å„ç§å‚æ•°å’Œæ¨¡å‹ç»“æ„ï¼Œå…·ä½“è§æ–‡ç« ç¬¬5é¡µã€‚

##### Sequence-to-sequence model

1. Use the open-sourced T5X framework to implement our transformer based encoder-decoder architecture.
2. Add semantic codewords (1024 (256Ã—4) tokens) to the vocabulary of the sequence-to-sequence model.
3. Add user-specific tokens (2000 tokens for user IDs) to the vocabulary.
	1. Use the Hashing Trick to map the raw user ID to one of the 2000 user ID tokens.
4. Construct the input sequence as the user Id token + the sequence of Semantic ID tokens corresponding to a given userâ€™s item interaction history

æ­¤å¤„çœç•¥å„ç§å‚æ•°å’Œæ¨¡å‹ç»“æ„ï¼Œå…·ä½“è§æ–‡ç« ç¬¬6é¡µã€‚

#### Performance on sequential recommendation (RQ1)

##### Baselines

- GRU4Rec is the first RNN-based approach that uses a customized GRU for the sequential recommendation task.
- Caser uses a CNN architecture for capturing high-order Markov Chains by applying horizontal and vertical convolutional operations for sequential recommendation.
- HGN: Hierarchical Gating Network (HGN) captures the long-term and short-term user interests via a new gating architecture.
- SASRec: Self-Attentive Sequential Recommendation (SASRec) uses a causal mask Transformer to model a user's sequential interactions.
- BERT4Rec: BERT4Rec addresses the limitations of unidirectional architectures by using a bi-directional self-attention Transformer for the recommendation task.
- FDSA: Feature-level Deeper Self-Attention Network (FDSA) incorporates item features in addition to the item embeddings as part of the input sequence in the Transformers.
- $S^3$-Rec: Self-Supervised Learning for Sequential Recommendation $\left(S^3-R e c\right)$ proposes pre-training a bi-directional Transformer on self-supervision tasks to improve the sequential recommendation.
- P5: P5 is a recent method that uses a pretrained Large Language Model (LLM) to unify different recommendation tasks in a single model.

##### Recommendation performance

æ€»ä½“å°±æ˜¯æ¯”åˆ«çš„æ¨¡å‹å¼º2%-30%

![[Table 1.png]]

#### Item representation (RQ2)

å¯¹æ¯”äº†å…¶ä»–çš„IDç”Ÿæˆæ–¹å¼ï¼ŒåŒ…æ‹¬éšæœºIDå’ŒLSH Semantic IDï¼Œå¥½ç”¨å°±å®Œäº‹äº†

![[Table 3.png]]

#### New capabilities (RQ3)

##### Cold-start recommendation

å’ŒKNNæ¯”è¾ƒå†·å¯åŠ¨æ€§èƒ½ï¼Œçº¸é¢ä¸Šè¶…è¿‡äº†KNNï¼Œä½†æ˜¯ä¼¼ä¹å¹¶æ²¡æœ‰é‚£ä¹ˆå¼ºã€‚

##### Recommendation diversity

![[Table 5.png]]

We report the Entropy@K for various temperature values in Table 5. We observe that temperature-sampling in the decoding stage can be effectively used to increase the diversity in the ground-truth categories of the items.

æ¸©åº¦å¾ˆé‡è¦

## Discussion

### Invalid IDs

åœ¨æ£€ç´¢å‰20é¡¹æ—¶ï¼Œåªæœ‰å¤§çº¦2%çš„IDæ— æ•ˆï¼Œé—®é¢˜ä¸å¤§ï¼Œæ•ˆæœå¾ˆå¥½ã€‚

### Effects of Semantic ID length and codebook size

ä¿®æ”¹codebookçš„å½¢çŠ¶å’Œå¤§å°å¯¹æ€§èƒ½å½±å“ä¸å¤§ï¼Œè¯´æ˜æ¨¡å‹å¾ˆå¥å£®ã€‚

### Inference cost

Beam searchå¤ªæ…¢äº†ï¼Œæ¯”KNNæ…¢å¾ˆå¤šã€‚

## Conclusion

é€šè¿‡å¯¹ä¸‰ä¸ªæ•°æ®é›†çš„å®éªŒï¼Œæˆ‘ä»¬è¡¨æ˜æˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥å®ç° SOTA ç»“æœå¹¶ä¸”å¯ä»¥å¾ˆå¥½åœ°æ³›åŒ–åˆ°æ–°çš„å’Œæœªè§è¿‡çš„é¡¹ç›®ã€‚ 

æˆ‘ä»¬çš„å·¥ä½œä¿ƒæˆäº†è®¸å¤šæ–°çš„ç ”ç©¶æ–¹å‘ã€‚ 

- æ¢ç´¢å¦‚ä½•å°†è¿™äº›è¯­ä¹‰ ID ä¸ LLM é›†æˆä»¥å¯ç”¨æ›´å¼ºå¤§çš„ä¼šè¯æ¨èæ¨¡å‹å°†ä¼šå¾ˆæœ‰è¶£
- æ¢è®¨å¦‚ä½•æ”¹è¿›è¯­ä¹‰ ID è¡¨ç¤ºä»¥åŠå¦‚ä½•ä½¿ç”¨å®ƒè¿›è¡Œæ’å
