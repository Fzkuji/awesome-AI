


主要还是用[[Special Multi-Task Learning|多任务学习模型]]

我们常见的多任务模型升级路线是Share-Bottom -> MMoE ->CGC/PLE

MMoE为了解决相关性较差的多任务联合训练，PLE引入专有expert降低跷跷板效应

[Mixture-of-Experts (MoE) 经典论文一览](https://zhuanlan.zhihu.com/p/542465517)