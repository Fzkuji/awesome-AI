# Temporal difference learning


## 基本概念介绍

Temporal Difference Learning (TD学习) 是一种预测未来回报的方法，它通过实际体验来不断调整预测。想象你在学习一个新游戏，开始时你不知道哪些动作会带来好结果，但随着不断尝试，你会逐渐了解什么时候你即将获胜。TD学习就是这样工作的。

### 核心思想

TD学习的核心思想非常简单：

> 根据新获得的信息来更新我们对未来的预测

具体来说，它基于"预测差异"来学习，即实际观察到的结果与之前预测的差异。

### 相关术语

在开始前，让我们了解几个简单术语：

- **状态(State)**: 环境的当前情况（比如游戏中的位置）
- **价值(Value)**: 从某个状态开始，预期能获得的总未来奖励
- **奖励(Reward)**: 执行动作后立即获得的反馈
- **学习率(Learning Rate)**: 决定新信息更新旧预测的速度

### 公式

TD学习的核心公式很直观：

```
新的价值估计 = 旧的价值估计 + 学习率 × (观察到的新信息 - 旧的价值估计)
```

用符号表示更简洁：

```
V(S) ← V(S) + α × [R + γ·V(S') - V(S)]
```

其中：
- `V(S)` 是当前状态的价值估计
- `α` 是学习率（一个小的正数，如0.1）
- `R` 是获得的即时奖励
- `γ` 是折扣因子（通常接近1，如0.9）
- `V(S')` 是下一个状态的价值估计
- `[R + γ·V(S') - V(S)]` 被称为"TD误差"

### 通俗理解

想象你在预测每天回家路上的交通时间：

1. 今天早上，你预测晚上回家需要30分钟
2. 晚上实际花了35分钟
3. 你会根据这个差异调整明天的预测："嗯，可能需要31-32分钟"

这就是TD学习的本质—根据实际体验不断调整预测。

### 工作流程

1. 初始化每个状态的价值估计（可以是随机值或零）
2. 观察当前状态S
3. 选择并执行一个动作
4. 获得奖励R，并进入新状态S'
5. 使用TD公式更新状态S的价值估计
6. 将S'设为当前状态
7. 重复步骤3-6直到学习完成

### 一个简单例子：小路径问题

考虑这个简单问题：
```
A → B → C → D → E(终点，奖励+1)
```

我们希望学习每个状态的价值（到达终点E的可能性）。

初始时，所有状态价值为0：V(A)=V(B)=V(C)=V(D)=V(E)=0

使用TD学习（α=0.1, γ=1）走几次这条路径后：

1. 第一次到达E时，更新D：
   - V(D) ← 0 + 0.1 × [1 + 1×0 - 0] = 0.1

2. 下一次，当从C到D时：
   - V(C) ← 0 + 0.1 × [0 + 1×0.1 - 0] = 0.01

3. 再多走几次后，价值会渐渐传播：
   - V(E) = 1 (终点固定价值)
   - V(D) ≈ 0.9 (非常接近终点)
   - V(C) ≈ 0.8 (距离终点更远)
   - V(B) ≈ 0.7
   - V(A) ≈ 0.6 (离终点最远)

### 优点

1. **实时学习**: 不需要等到任务结束，每一步都可以学习
2. **适应性强**: 可以从不完整的体验中学习
3. **计算效率**: 不需要存储完整的经验历史
4. **处理随机环境**: 通过多次体验平均化噪声




## 主要类型


时序差分(Temporal Difference, TD)方法是强化学习中的核心算法家族，结合了动态规划与蒙特卡罗方法的优势。TD方法通过部分经验进行学习，无需等待整个回合结束就能更新价值估计。本文详细介绍TD方法的主要类型及其特点。

### Sarsa算法

Sarsa是一种经典的on-policy(在策略)TD控制方法。

#### 核心特点

- 名称来源于更新使用的序列：State, Action, Reward, next State, next Action (S,A,R,S',A')
- 直接学习状态-动作值函数Q(s,a)
- 使用当前策略生成的经验进行学习

#### 更新公式

txt

```
Q(s,a) ← Q(s,a) + α[r + γ·Q(s',a') - Q(s,a)]
```

其中α是学习率，γ是折扣因子，r是即时奖励。

#### 算法步骤

1. 在当前状态s选择动作a（通常使用ε-贪婪策略）
2. 执行动作a，观察奖励r和下一状态s'
3. 在状态s'选择下一个动作a'
4. 使用(s,a,r,s',a')更新Q值表
5. s←s'，a←a'
6. 重复直到学习结束

#### 优缺点

- **优点**：更加保守和安全，考虑了探索行为的影响
- **缺点**：收敛可能较慢，特别是在探索较多的场景

### Q-learning

Q-learning是最流行的off-policy(离策略)TD控制方法。

#### 核心特点

- 直接学习最优动作值函数Q*，而不依赖于当前策略
- 可以在使用探索策略的同时学习最优策略

#### 更新公式

txt

```
Q(s,a) ← Q(s,a) + α[r + γ·max_a'Q(s',a') - Q(s,a)]
```

#### 算法步骤

1. 在当前状态s选择动作a（通常使用ε-贪婪策略）
2. 执行动作a，观察奖励r和下一状态s'
3. 更新Q值表，使用下一状态的最大Q值（而非实际选择的动作）
4. s←s'
5. 重复直到学习结束

#### 优缺点

- **优点**：直接逼近最优策略，通常比Sarsa收敛更快
- **缺点**：可能过于乐观，在某些场景下不如Sarsa安全

### TD(λ)

TD(λ)是一种将单步TD方法扩展到多步的方法。

#### 核心特点

- 通过参数λ∈[0,1]平滑地结合单步TD和蒙特卡罗方法
- λ=0时等价于单步TD，λ=1时等价于蒙特卡罗方法
- 常通过资格迹(eligibility traces)实现

#### 主要变体

- **Sarsa(λ)** ：Sarsa的多步回报版本
- **Q(λ)** ：Q-learning的多步回报版本

#### 优缺点

- **优点**：结合immediate bootstrapping和长期回报，通常比单步TD更有效
- **缺点**：计算复杂度增加

### Expected Sarsa

Expected Sarsa是Sarsa的一个变体，性能通常优于标准Sarsa。

#### 核心特点

- 使用下一步所有可能动作的期望Q值，而非单一采样
- 可以实现为on-policy或off-policy算法

#### 更新公式

txt

```
Q(s,a) ← Q(s,a) + α[r + γ·∑_a'π(a'|s')Q(s',a') - Q(s,a)]
```

#### 优缺点

- **优点**：通常有更低的方差，学习更稳定
- **缺点**：计算量略大，需要知道策略的概率分布

### Double Q-learning

Double Q-learning旨在解决标准Q-learning中的过度估计问题。

#### 核心特点

- 维护两个独立的Q值函数
- 一个用于选择动作，另一个用于评估动作

#### 更新公式

txt

```
如果选择更新A：
    Q_A(s,a) ← Q_A(s,a) + α[r + γ·Q_B(s',argmax_a'Q_A(s',a')) - Q_A(s,a)]
```

#### 优缺点

- **优点**：减少过度估计，提高学习稳定性
- **缺点**：计算和内存需求增加

### 现代TD方法

近年来，随着深度学习的发展，出现了许多基于TD的先进算法：
#### DQN (Deep Q-Network)

- 结合深度神经网络与Q-learning
- 使用经验回放和目标网络稳定训练

#### TD3 (Twin Delayed DDPG)

- 连续动作空间的TD方法
- 使用双Q网络和延迟策略更新

#### 分布式TD方法

- 学习价值分布而非单一期望值
- 包括C51、QR-DQN、IQN等算法

### 总结

时序差分方法是强化学习的核心，提供了一种从经验中学习的高效框架。各种TD方法在不同任务中各有优势：

- **Sarsa**：适用于需要考虑探索风险的场景
- **Q-learning**：适合寻找最优策略的场景
- **TD(λ)** ：平衡短期与长期回报的场景
- **Expected Sarsa**：需要降低方差、稳定学习的场景
- **Double Q-learning**：防止价值过度估计的场景
- **现代TD方法**：复杂高维状态空间问题

选择适当的TD方法应考虑具体问题特性、计算资源限制以及安全性要求。







## 总结

Temporal Difference Learning是一种强大而直观的学习方法，它通过比较预测和实际结果之间的差异来不断改进。这种方法模仿了人类学习的一个重要方面：我们不断根据新体验调整对未来的预期。

记住TD学习的核心：
- 预测未来奖励
- 观察实际结果
- 调整预测
- 重复这个过程

掌握了这个简单但强大的概念，你就掌握了强化学习中一个最重要的工具！