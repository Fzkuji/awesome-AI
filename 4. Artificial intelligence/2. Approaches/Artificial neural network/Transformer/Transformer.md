# Transformer



## Related works


### 模型设计

看这些论文的时候我在思考transformer的加速推理问题，然后思考transformer的层数问题。但是很快我就发现了一些针对深度学习模型的加速硬件，可以把推理速度提升几百倍（比如[groq](https://groq.com)），于是感觉架构的思考也没什么必要了，反正计算速度问题都会通过硬件解决。

schuBERT: Optimizing Elements of BERT

A Study on Transformer Configuration and Training Objective

Experiments And Discussions On Vision Transformer (ViT) Parameters For Object Tracking

Rethinking the role of scale for in-context learning: An interpretability-based case study at 66 billion scale














