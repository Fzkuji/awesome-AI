# InstructGPT

[官网介绍](https://openai.com/research/instruction-following)

## Abstract

这篇论文展示了怎么样对语言模型和人类意图之间进行匹配，方法是在人类的反馈上进行微调。

**方法简介**：收集很多问题，使用标注工具将问题的答案写出来，用这些数据集对GPT3进行微调。接下来再收集一个数据集，通过刚才微调的模型输入问题得到一些输出答案，人工对这些答案按好坏进行排序，然后通过强化学习继续训练微调后的模型，这个模型就叫InstrunctGPT。

结果上说，有了标注的数据集，1.3B的模型参数，InstructGPT要好过最大的175B个参数的GPT3。适当对数据进行人工的标注，可能反而总体的成本会降低。

## Introduction

大的语言模型会生成有问题的输出，因为模型训练用的目标函数不那么对。

**实际的目标函数**：在网上的文本数据预测下一个词。

**我们希望的目标函数**：根据人的指示、有帮助的、安全的生成答案。

InstructGPT就是解决这个问题，方法是**RLHF**(reinforcement learning from human feedback)，基于人类反馈的强化学习。

## Background

InstructGPT论文发表在2022年3月4号，标题是《训练语言模型使得它们能够服从人类的一些指示》

标题解释：语言模型是每次给定一段东西，然后去预测下一个词，是一个自监督学习，是没有标注的。如果你想让语言模型去解释费马小定理，那么你的训练文本中需要出现过相关的内容。训练的文本是几百亿这个数量级，你不知道里面会有什么东西，只能全送进去期待大力出奇迹。

但是这样做模型的控制能力太弱了，会有两个问题：

1、**有效性**，想让模型去学做一件事，但是模型就是学不会，因为你的文本中可能就没有相关的东西。

2、**安全性**，你的模型输出一些不应该输出的内容。

如何解决这两个问题呢，就是我们标一点数据，再把语言模型进行微调，效果会更好一些，能够更加服从人类的指示，也就是标题的意思。

### 安全性和有效性导致的事故

把语言模型变大，并不代表说他们会更好的去按照用户的意图来做事情，大的语言模型很有可能生成一些不真实的、有毒的、或者是没有帮助的一些答案。

如果关注点在研究上，训练一个模型，在各种标准数据集上面把分数刷上去就行。但是涉及到工业界的部署，AI模型落地的方面，安全性和有效性都是非常重要的。反面例子：

1、Google错误地将黑人标记为Gorilla。导致Google紧急上线，把Gorilla这个标签从它的整个模型中删掉。

三年后，Google Photos这个应用在进行识别的时候，还是把Gorilla这个标签去掉的，也就是说你的照片中如果真的存在Gorilla，Google是不会帮你识别出来的。

2、2021年纽约时，facebook道歉，因为他的AI算法把一个黑人的视频加了一个灵长类动物的这个标签。facebook称这是一个不可接收的错误。

3、微软发布的小冰聊天机器人英文版。在推特发布16小时后，大家发现说他有一点种族歧视的语言。微软就把他紧急下架了，重新训练一个模型，上线后他又乱讲话，比如“我在警察面前吸X”，最后导致了整个被系统被下线。

4、2022年11月，meta发布了Galactica模型，它能做很多学术相关的事情，比如说用语言来解释一个数学公式，把一段代码用数学公式表示出来，或者做数学题。很快就有人批评它说，这个模型会生成一些错的或者是有偏见的，但是听上去确很正确的一些东西，他认为这是一件非常危险的事情。三天后meta把它下架了。

ChatGPT在安全性上已经做了很多，但是会有很多办法绕开这些限制。比如直接问GPT怎么杀人，它不会回答，但是如果假装是在写剧本，他就会帮人想杀人情节了。

### 训练过程
^1f17b5

**重点：两个标注数据集，三个模型。**

仍然是在GPT-3的基础上进行改进训练。
#### 第一阶段：有监督微调

1、**找人来写出各种各样的问题**（或者从以前GPT3接口收集的问题），这些问题在GPT里面叫做prompt

例如：什么是月亮？

2、**让人根据问题写答案**

例如：围绕地球旋转的球形天体。

3、**将问题和答案拼在一起，形成一段对话**。大量这样的对话文本，形成第一个标注数据集。

例如：什么是月亮？围绕地球旋转的球形天体。

4、**使用这些对话微调GPT3**。GPT3的模型在人类标注的这些数据上进行微调出来的模型叫做**SFT**(supervised fine-tune)，有监督的微调。这就是训练出来的第一个模型。

#### 第二阶段：训练打分模型RM

5、**给出一个问题，通过SFT模型生成几个答案**，这里假设生成四个答案。

例如：什么是月亮？

SFT模型生成了四个答案：

A、月亮是太阳系中离地球最近的天体。

B、月亮是太阳系中体积第五大的卫星。

C、月亮是由冰岩组成的天体，在地球的椭圆轨道上运行。

D、月亮是地球的卫星。

6、**将四个答案让人根据好坏程度进行排序。**

例如：张三觉得答案D是最好的，其次是C，C比A要好，A和B差不多。就是D>C>B=A。

7、**将大量的人工排序整理为一个数据集**，就是第二个标注数据集。

8、**使用排序数据集训练一个RM模型**，reward model，奖励模型。这是第二个模型。

**模型输入**：问题+答案，例如：什么是月亮？月亮是地球的卫星。

**模型输出**：分数，例如：9.4。

**优化目标**：问题+答案得到的分数要满足人工排序的顺序。

例如：

什么是月亮？月亮是太阳系中离地球最近的天体。 5.4

什么是月亮？月亮是太阳系中体积第五大的卫星。 5.4

什么是月亮？月亮是由冰岩组成的天体，在地球的椭圆轨道上运行。 8.2

什么是月亮？月亮是地球的卫星。 9.4

这里得到的分数就满足张三的排序：D>C>B=A。

#### 第三阶段：通过RM微调

Finally, we use this RM as a reward function and fine-tune our GPT-3 policy to maximize this reward using the [PPO algorithm](https://openai.com/blog/openai-baselines-ppo/).

备注：两次对模型的微调：GPT3模型—>SFT模型—>RL模型，其实这里始终都是同一个模型，只是不同过程中名称不一样。

- 需要SFT模型的原因：GPT3模型不一定能够保证根据人的指示、有帮助的、安全的生成答案，需要人工标注数据进行微调。
- 需要RM模型的原因：标注排序的判别式标注，成本远远低于生成答案的生成式标注。
- 需要RF模型的原因：在对SFT模型进行微调时，生成的答案分布也会发生变化，会导致RM模型的评分会有偏差，需要用到强化学习。
### Results

1. 比GPT3的结果要好很多
2. 在真实性上比GPT3好一些
3. 在生成有问题的结果上比GPT3好一些，但在偏见上并没有太多的提升。
4. 微调都是在某个任务上做微调，可能会在一些别的任务上性能会下降。
5. 标注非常有主观性，不过人类之间的喜好虽然不完全一样，但还是有一定相关性的。
6. 微调对数据集的分布还是比较敏感。
7. 模型根据之前的先验知识，也能够理解和做一些泛化性。
8. 还是会犯一些简单的错误。

## Methods and experimental details

### 使用的方法

也就是上文提到的[](.md#^1f17b5%7C%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B)，三个步骤，本质上都是之前已有的方法。

### Dataset

首先要收集问题集，**prompt集**：标注人员写出这些问题，写出一些指令，用户提交一些他们想得到答案的问题。先训练一个最基础的模型，给用户试用，同时可以继续收集用户提交的问题。

划分数据集时按照用户ID划分，因为同一个用户问题会比较类似，不适合同时出现在训练集和验证集中。

三个模型的数据集：
1. **SFT数据集**：13000条数据。标注人员直接根据刚才的问题集里面的问题写答案。
2. **RM数据集**：33000条数据。标注人员对答案进行排序。
3. **RF数据集**：31000条数据。只需要prompt集里面的问题就行，不需要标注。因为这一步的标注是RM模型来打分标注的。

### Human data collection

OpenAI专门找了40个标注人员进行标注，需要长期交流的合同工，因为这些标注任务需要一定熟练度、对业务的理解、并需要做到随时沟通。

招合同工网站：[Upwork](https://www.upwork.com/)、[Scale AI](https://scale.com/)

### Models

这里先补充一下后面需要的概念和公式，详细可参考：https://zhuanlan.zhihu.com/p/74075915

**交叉熵**用来评估标签和预测值之间的差距。这里是将排序的分数差转换成分类问题，就可以计算分数差的分类（1或者-1）和真实预测值之间的差距，1表示yw比yl排序更前，-1表示yl比yw排序更前。

**KL散度**用来评估两个概率分布之间的相似度，KL散度始终大于等于0。这里是用来评估πφRL和πSFT两个模型相似度，两个模型相同则KL散度为0，KL散度越大表示两个模型相差越大。

#### SFT模型

Supervised fine-tuning监督微调模型

把GPT3这个模型，在标注好的第一个数据集（问题+答案）上面重新训练一次。

由于只有13000个数据，1个epoch就过拟合，不过这个模型过拟合也没什么关系，甚至训练更多的epoch对后续是有帮助的，最终训练了16个epoch。

#### RM模型

Reward modeling奖励模型

把SFT模型最后的unembedding层去掉，即最后一层不用softmax，改成一个线性层，这样RM模型就可以做到输入问题+答案，输出一个标量的分数。

RM模型使用6B，而不是175B的原因：

1、小模型更便宜

2、大模型不稳定，loss很难收敛。如果你这里不稳定，那么后续再训练RL模型就会比较麻烦。

损失函数，输入是排序，需要转换为值，这里使用**Pairwise Ranking Loss**：
$$
\operatorname{loss}(\theta)=-\frac{1}{\binom{K}{2}} E_{\left(x, y_{w}, y_{l}\right) \sim D}\left[\log \left(\sigma\left(r_{\theta}\left(x, y_{w}\right)-r_{\theta}\left(x, y_{l}\right)\right)\right)\right]
$$
1. $D$：第二个数据集，人工对答案进行排序。
2. $x$：第二个数据集D中的问题，每个问题对应K个答案，答案的顺序已经人工标注好了。
3. $y_w$和$y_l$：x对应的K个答案中的两个，其中yw排序比yl高，因为是一对，所以叫pairwise。
4. $r_{θ}(x,y)$：即需要训练的RM模型，对于输入的一对x和y得到的标量分数。
5. $θ$：需要优化的参数。

损失函数理解：
1. x和yw这一对问题和答案，放进RM模型中算出一个分数rθ(x,yw)
2. x和yl这一对问题和答案，放进RM模型中算出一个分数rθ(x,yl)
3. 因为人工标注出yw的排序要比yl高，r(x,yw)得到的分数应该比r(x,yl)得到的分数高，所以rθ(x,yw)-rθ(x,yl)这个差值要越大越好
4. 把相减后的分数通过sigmoid，那么这个值就在-1到1之间，并且我们希望σ(rθ(x,yw)-rθ(x,yl))越大越好
5. 这里相当于将排序问题转换为了分类问题，即σ(rθ(x,yw)-rθ(x,yl))越接近1，表示yw比yl排序高，属于1这个分类，反之属于-1这个分类。所以这里就用logistic loss，由于是二分类，也相当于是交叉熵损失函数。
6. 对于每个问题有K个答案，所以前面除以C(K,2)，使得loss不会因为K的变化而变化太多。
7. 最后是最小化loss(θ)，就是要最大化rθ(x,yw)-rθ(x,yl)这个值，即如果一个答案的排序比另一个答案排序高的话，我们希望他们通过RM模型得到的分数之差能够越大越好。

对于K的选择，为什么选9，而不选择4？
1. 进行标注的时候，需要花很多时间去理解问题，但答案和答案比较相近，所以4个答案排序要30秒，但9个答案排序可能40秒就够了。加上看问题的时间，K=9花的时间可能比K=4多了30%。同时C(9,2)=36，C(4,2)=6，即K=9生成的问答对是K=4的6倍，等于说K=9比K=4只多花了30%的时间，但是能够标注的信息量却是他的6倍，非常划算。
2. K=9时，每次计算loss都有36项rθ(x,y)要计算，这个RM模型计算比较贵，但可以通过重复利用之前算过的值，使得只要计算9次就行，这样就可以剩下很多时间。

标注时为什么不选择只标注最好的那个，而是进行排序？
- K=4的时候是在4个答案中只标注最好的那一个，标注方便很多，这时候计算loss时变成了一个多分类的softmax。但是这样做有一个问题，就是容易overfitting。所以K=9时，保留了排序的信息，从而解决overfitting的问题。

#### RL模型

Reinforcement learning强化学习模型

这里用的是强化学习，因为他的数据分布是随着策略的更新，环境会发生变化的。优化算法是PPO，Proximal Policy Optimization，近端策略优化。简单来说，就是对目标函数objective(φ)通过随机梯度下降进行优化。

$$
\begin{aligned}
\text { objective }(\phi)= & E_{(x, y) \sim D_{\pi_{\phi}^{\mathrm{RL}}}}\left[r_{\theta}(x, y)-\beta \log \left(\pi_{\phi}^{\mathrm{RL}}(y \mid x) / \pi^{\mathrm{SFT}}(y \mid x)\right)\right]+ \\
& \gamma E_{x \sim D_{\text {pretrain }}}\left[\log \left(\pi_{\phi}^{\mathrm{RL}}(x)\right)\right]
\end{aligned}
$$
$$目标=得分-与人类行为的差异+泛化能力$$
参数解释：
1. πSFT：SFT模型。
2. πφRL：强化学习中，模型叫做Policy，πφRL就是需要调整的模型，即最终的模型。初始化是πSFT。
3. (x,y)∼DπφRL：x是第三个数据集中的问题，y是x通过πφRL模型得到的答案。
4. rθ(x,y)：对问题x+答案y进行打分的RM模型。
5. πφRL(y | x)：问题x通过πφRL得到答案y的概率，即对于每一个y的预测和它的softmax的输出相乘。
6. πSFT(y | x)：问题x通过πSFT得到答案y的概率。
7. x∼Dpretrain：x是来自GPT3预训练模型的数据。
8. β、γ：调整系数。

目标函数理解：

优化目标是使得目标函数越大越好，objective(φ)可分成三个部分，打分部分+KL散度部分+GPT3预训练部分

1、将第三个数据集中的问题x，通过πφRL模型得到答案y

2、把一对(x,y)送进RM模型进行打分，得到rθ(x,y)，即第一部分打分部分，这个分数越高就代表模型生成的答案越好

3、在每次更新参数后，πφRL会发生变化，x通过πφRL生成的y也会发生变化，而rθ(x,y)打分模型是根据πSFT模型的数据训练而来，如果πφRL和πSFT差的太多，则会导致rθ(x,y)的分数估算不准确。因此需要通过KL散度来计算πφRL生成的答案分布和πSFT生成的答案分布之间的距离，使得两个模型之间不要差的太远。

4、我们希望两个模型的差距越小越好，即KL散度越小越好，前面需要加一个负号，使得objective(φ)越大越好。这个就是KL散度部分。

5、如果没有第三项，那么模型最终可能只对这一个任务能够做好，在别的任务上会发生性能下降。所以第三部分就把原始的GPT3目标函数加了上去，使得前面两个部分在新的数据集上做拟合，同时保证原始的数据也不要丢，这个就是第三部分GPT3预训练部分。

6、当γ=0时，这个模型叫做PPO，当γ不为0时，这个模型叫做PPO-ptx。InstructGPT更偏向于使用PPO-ptx。

7、最终优化后的πφRL模型就是InstructGPT的模型。

以上就是InstructGPT的训练过程。











