RLHF，即Reinforcement learning from human feedback（基于人类反馈的强化学习）


2023-06-15
- [最新RLHF拯救语言模型「胡说八道」！微调效果比ChatGPT更好，两名华人共同一作](https://mp.weixin.qq.com/s/iqf6Tw2iyYNAUoAj3f1MNw)

2023-08-05
[RLHF缺陷完整揭示！MIT哈佛等32位学者联合发布](https://mp.weixin.qq.com/s/NgR_MlGKatetCSzrQH2UUg)













