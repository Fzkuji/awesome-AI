# Memorizing Transformer的详细介绍

Memorizing Transformer是一种创新的语言模型架构，它通过添加外部记忆机制来显著增强标准Transformer处理长文本的能力。以下是对其工作原理、架构设计和性能的详细介绍：

## 核心思想

记忆化Transformer的核心思想是让模型能够"记住"之前见过的内容，并在需要时检索相关信息。这种能力对于处理长文档特别重要，例如：
- 小说中的角色和事件可能跨越多个章节被引用
- 源代码中的类和函数定义可能远离它们被调用的位置
- 数学证明中需要使用先前定义的引理

## 架构设计

记忆化Transformer在标准Transformer架构的基础上做了以下扩展：

1. **外部记忆机制**：模型维护一个大型的外部记忆，存储之前见过的序列的(key, value)对
   
2. **kNN增强的注意力机制**：
   - 使用k-最近邻(kNN)查找算法在外部记忆中检索相关信息
   - 这种查找是非微分的，意味着梯度不会反向传播到外部记忆中
   - 这一特性使得模型可以重用之前计算过的keys和values，大大减少了计算量

3. **模型结构**：
   - 基础部分是一个标准的仅解码器Transformer
   - 输入文本被分词并嵌入到向量空间
   - 这些嵌入向量通过一系列Transformer层处理
   - 每一层包含自注意力机制和前馈网络(FFN)
   - 在某些层中添加了kNN查找机制，使模型能够访问外部记忆

4. **训练过程**：
   - 长文档被分割成512个token的子序列
   - 与标准做法不同，这些子序列不会被打乱，而是按顺序输入到Transformer中
   - 每处理完一个子序列后，其(key, value)对会被添加到外部记忆中

## 性能提升

记忆化Transformer在多个长文本数据集上展现出显著的性能提升：

1. **困惑度改进**：
   - 在各种数据集上（包括C4网页文本、arXiv数学论文、PG-19书籍、GitHub代码和Isabelle形式证明）都显示出困惑度的大幅降低
   - 性能提升相当于将标准Transformer的参数量增加5倍

2. **记忆大小的影响**：
   - 随着外部记忆大小的增加，模型性能稳步提升
   - 研究表明，记忆大小可以扩展到262K个token，仍然能带来性能提升
   - 即使是在8B参数的大模型上，添加外部记忆仍然能带来显著收益

3. **微调能力**：
   - 已有的预训练Transformer模型可以通过微调来使用外部记忆
   - 在仅仅4%的预训练时间内，微调模型就能弥补85%的性能差距

4. **实际应用案例**：
   - 在Isabelle数学证明数据集上，模型能够查找并使用之前定义的引理和数学对象
   - 在10个预测引理名称的样例中，有8次模型成功找到了它需要预测的引理的定义
   - 在arXiv数学论文和GitHub代码中，模型能够检索函数和变量名称

## 信息检索模式

研究发现，记忆化Transformer主要在以下情况下从外部记忆中获益：
- 查找罕见词汇，如专有名词、引用、引文和函数名
- 当名称的首次使用与后续使用之间的距离太远，无法适应本地上下文时

总的来说，记忆化Transformer通过简单而有效的kNN查找机制，显著扩展了语言模型的上下文长度，使模型能够"记住"并利用之前见过的信息，从而在长文本处理任务上取得了突破性的进展。