In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs.

## Classification

### Ridge activation functions

Ridge functions are multivariate functions acting on a linear combination of the input variables.

e.g.
- [Linear](https://en.wikipedia.org/wiki/Linear "Linear") activation: $\phi (\mathbf {v} )=a+\mathbf {v} '\mathbf {b}$,
- [ReLU](https://en.wikipedia.org/wiki/ReLU "ReLU") activation: $\phi (\mathbf {v} )=\max(0,a+\mathbf {v} '\mathbf {b} )$,
- [Heaviside](https://en.wikipedia.org/wiki/Heaviside_function "Heaviside function") activation: $\phi (\mathbf {v} )=1_{a+\mathbf {v} '\mathbf {b} >0}$,
- [Logistic](https://en.wikipedia.org/wiki/Logistic_function "Logistic function") activation: $\phi (\mathbf {v} )=(1+\exp(-a-\mathbf {v} '\mathbf {b} ))^{-1}$.

> 这些函数有助于对输入和输出之间更复杂的关系进行建模
> ——ChatGPT

> 岭函数不易受维数灾难的影响
> ——Wikipedia

### Radial activation functions

Radial activation functions are characterized by their radial symmetry, which means that their output depends only on the distance of the input from the origin (center).

e.g.
-   [Gaussian](https://en.wikipedia.org/wiki/Gaussian_function "Gaussian function"): $\,\phi (\mathbf {v} )=\exp \left(-{\frac {\|\mathbf {v} -\mathbf {c} \|^{2}}{2\sigma ^{2}}}\right)$
-   Multiquadratics: $\,\phi (\mathbf {v} )={\sqrt {\|\mathbf {v} -\mathbf {c} \|^{2}+a^{2}}}$
-   Inverse multiquadratics: $\,\phi (\mathbf {v} )=\left(\|\mathbf {v} -\mathbf {c} \|^{2}+a^{2}\right)^{-{\frac {1}{2}}}$
-   [Polyharmonic splines](https://en.wikipedia.org/wiki/Polyharmonic_spline "Polyharmonic spline")

> 对于函数逼近、插值和模式识别等任务很有用，因为它们可以充当局部特征检测器
> ——ChatGPT

### Folding activation functions

Folding activation functions are extensively used in the [pooling layers](https://en.wikipedia.org/wiki/Convolutional_neural_network#Pooling_layers "Convolutional neural network") in [convolutional neural networks](https://en.wikipedia.org/wiki/Convolutional_neural_network "Convolutional neural network"), and in output layers of multiclass classification networks. These activations perform aggregation over the inputs, such as taking the [mean](https://en.wikipedia.org/wiki/Mean "Mean"), [minimum](https://en.wikipedia.org/wiki/Minimum "Minimum") or [maximum](https://en.wikipedia.org/wiki/Maximum "Maximum"). In multiclass classification the [softmax](https://en.wikipedia.org/wiki/Softmax_function "Softmax function") activation is often used.

???

## Lists

### Swish

Introduced by Ramachandran et al. in [Searching for Activation Functions](https://paperswithcode.com/paper/searching-for-activation-functions)

**Swish** is an activation function:
$$\operatorname {swish} (x)=x \cdot \operatorname{sigmoid} (\beta x)={\frac {x}{1+e^{-\beta x}}}.$$
where $\beta$ is a learnable parameter. Nearly all implementations do not use the learnable parameter $\beta$, in which case the activation function is $x\sigma(x)$ ("Swish-1").

他们的实验表明，**在跨越多个具有挑战性的数据集的更深层模型上，Swish往往比ReLU表现更好**。据信，改进的原因之一是swish函数有助于缓解梯度消失问题。

### Gated Linear Unit

Related papers:  
* TabNet: [https://arxiv.org/abs/1908.07442](https://arxiv.org/abs/1908.07442)  
* Language modeling with Gated Convolutional Networks: [https://arxiv.org/abs/1612.08083](https://arxiv.org/abs/1612.08083)

GLUs are designed to help control the flow of information through the network, allowing it to learn long-range dependencies more effectively.

Mathematically, given an input vector $x$, a GLU can be represented as:
$$
\operatorname{GLU}(x, W, V, b, c)=\sigma(x W+b) \otimes (x V+c)
$$
where:
- $W$ and $V$ are weight matrices
- $b$ and $c$ are bias vectors
- $σ$ denotes the sigmoid function
- $\otimes$ denotes element-wise multiplication

One naive way to [implement](https://github.com/hermesdt/machine_learning/blob/master/GLU.ipynb) this is:
```python
class GLU(nn.Module):
    def __init__(self, in_size):
        super().__init__()
        self.linear1 = nn.Linear(in_size, in_size)
        self.linear2 = nn.Linear(in_size, in_size)
    
    def forward(self, X):
        return self.linear1(X) * self.linear2(X).sigmoid()
```

Making it faster:
```python
class FastGLU(nn.Module):
    def __init__(self, in_size):
        super().__init__()
        self.in_size = in_size
        self.linear = nn.Linear(in_size, in_size*2)
    
    def forward(self, X):
        out = self.linear(X)
        return out[:, :self.in_size] * out[:, self.in_size:].sigmoid()
```


### SwiGLU
^ceeab8

SwiGLU is a combination of Swish and GLU activation functions. SwiGLU is defined as follows:
$$
\begin{align} 
\operatorname{SwiGLU}(x, W, V, b, c, \beta)
&=\operatorname{Swish}_\beta(x W+b) \otimes (x V+c) \\ 
&= (x W+b) \cdot \sigma(\beta (x W+b))) \otimes (x V+c)
\end{align}
$$

The code (Generated by ChatGPT) is:
```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SwiGLU(nn.Module):
    def __init__(self, in_size, beta=1.0):
        super(SwiGLU, self).__init__()
        self.linear1 = nn.Linear(in_size, in_size)
        self.linear2 = nn.Linear(in_size, in_size)
        self.beta = beta

    def swish(self, x):
        return x * F.sigmoid(self.beta * x)

    def forward(self, X):
        return self.swish(self.linear1(X)) * self.linear2(X)

# Example usage
# input_size = 128
# swiglu = SwiGLU(input_size)
# input_tensor = torch.randn(1, input_size)
# output = swiglu(input_tensor)
# print(output.shape)
```

Used in PaLM & [[LLaMA]].







