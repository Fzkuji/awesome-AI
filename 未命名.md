## 📝 重新组织的段落建议

这里是一个更清晰的表述：

---

To address this limitation, we propose two key innovations that work synergistically:

**First**, we introduce **Negative Value Masking (NVM)**, a variant of ReLU that operates in the log-domain:

$$\tilde{s}_i = \begin{cases} s_i & \text{if } s_i \geq 0 \ -\infty & \text{if } s_i < 0 \end{cases}$$

Unlike standard ReLU which maps negative values to zero, NVM maps them to negative infinity, effectively **eliminating** these positions entirely when combined with softmax. This hard masking ensures that positions with negative attention scores are completely removed from consideration rather than merely suppressed.

**Second**, we propose **Elastic-Softmax**, which combines NVM with learnable parameters to create an adaptive attention mechanism:

$$\text{ElasticSoftmax}(\mathbf{s})_i = \frac{\exp(\tilde{s}_i / \tau)}{\sum_{j=1}^{n} \exp(\tilde{s}_j / \tau) + \beta} \quad \text{for } i = 1, \ldots, n$$

where $\tilde{s}_i$ is the output of NVM, $\tau$ is a learnable temperature parameter controlling the sharpness of the distribution, and $\beta$ is a learnable offset parameter that prevents saturation and enables fine-grained control over the attention weights.

By combining these two components, Elastic-Softmax achieves both **hard sparsity** (through NVM) and **adaptive flexibility** (through learnable parameters), allowing the model to dynamically determine both which positions to attend to and how to distribute attention among them.

---

## 🔄 或者更简洁的版本：

---

To address this limitation, we propose combining two complementary techniques:

**Negative Value Masking (NVM)**: A ReLU variant for the log-domain that maps negative values to $-\infty$ instead of zero: $$\tilde{s}_i = \begin{cases} s_i & \text{if } s_i \geq 0 \ -\infty & \text{if } s_i < 0 \end{cases}$$

This ensures complete elimination of negative attention scores rather than soft suppression.

**Elastic-Softmax**: An adaptive attention mechanism that combines NVM with learnable parameters: $$\text{ElasticSoftmax}(\mathbf{s})_i = \frac{\exp(\tilde{s}_i / \tau)}{\sum_{j=1}^{n} \exp(\tilde{s}_j / \tau) + \beta}$$

where $\tau$ (temperature) and $\beta$ (offset) are learnable parameters that allow the model to adaptively control attention sharpness and prevent saturation.

Together, these innovations enable both hard sparsity and adaptive attention distribution.

---

关键改进：

1. ✅ 明确提出是**两个创新**的结合
2. ✅ NVM作为独立贡献，强调是ReLU变体
3. ✅ Elastic-Softmax作为组合方案
4. ✅ 清楚说明每个组件的作用


```python
def softmax_with_eps(self, x):
    x = np.maximum(x, -1e10)  # 限制最小值
    exp_x = np.exp(x)
    return exp_x / (np.sum(exp_x) + self.eps)
```


哦，我理解了！你想要的是：**如果原始输入x小于0，则变成-inf；否则保持原值**。这相当于ReLU的变体。

## 正确的数学公式


$$\text{output}_i = \frac{\exp(\text{ReLU}_{\infty}(x_i))}{\sum_{j} \exp(\text{ReLU}_{\infty}(x_j)) + \epsilon}$$

其中 $$\text{ReLU}_{\infty}(x) = \begin{cases} x & \text{if } x \geq 0 \\ -\infty & \text{if } x < 0 \end{cases}$$

## 代码实现

```python
import numpy as np
import torch

def relu_inf_softmax(x, eps=1e-10):
    """
    将负值变为-inf，非负值保持不变，然后应用带epsilon的softmax
    """
    # Step 1: 负值变-inf，非负值保持
    masked_x = np.where(x >= 0, x, -np.inf)
    
    # Step 2: 带epsilon的softmax
    exp_x = np.exp(masked_x)
    return exp_x / (np.sum(exp_x) + eps)

# PyTorch版本
def relu_inf_softmax_torch(x, eps=1e-10):
    # 负值变-inf
    masked_x = torch.where(x >= 0, x, float('-inf'))
    
    # Softmax with epsilon
    exp_x = torch.exp(masked_x)
    return exp_x / (torch.sum(exp_x) + eps)

# 数值稳定版本
def stable_relu_inf_softmax(x, eps=1e-10):
    """
    数值稳定版本，处理全负值情况
    """
    mask = x >= 0
    
    # 检查是否全部为负值
    if not np.any(mask):
        # 全负值时返回均匀分布（或全零，取决于需求）
        return np.ones_like(x) / len(x)
    
    # 负值变-inf
    masked_x = np.where(mask, x, -np.inf)
    
    # 数值稳定的softmax（减去最大值）
    max_val = np.max(masked_x[mask])
    exp_x = np.exp(masked_x - max_val)
    
    return exp_x / (np.sum(exp_x) + eps)
```

## 示例

```python
x = np.array([-2.0, 3.0, -1.0, 5.0, 0.0, 2.0])

result = relu_inf_softmax(x)
print(f"输入: {x}")
print(f"掩码后: {np.where(x >= 0, x, -np.inf)}")
print(f"输出: {result}")
print(f"和: {np.sum(result)}")

# 输入: [-2.  3. -1.  5.  0.  2.]
# 掩码后: [-inf   3. -inf   5.   0.   2
```