Auto-regressive Language Models (LMs) assign significant attention to the first
token, even if it is not semantically important, which is known as attention sink.
This phenomenon has been widely adopted in applications such as streaming/long
context generation, KV cache optimization, inference acceleration, model
quantization, and others. Despite its widespread use, a deep understanding of
attention sink in LMs is still lacking. In this work, we first demonstrate that
attention sinks exist universally in auto-regressive LMs with various inputs, even
in small models. Furthermore, attention sink is observed to emerge during the
LM pre-training, motivating us to investigate how optimization, data distribution,
loss function, and model architecture in LM pre-training influence its emergence.
We highlight that attention sink emerges after effective optimization on sufficient
training data. The sink position is highly correlated with the loss function and data
distribution. Most importantly, we find that attention sink acts more like key biases,storing extra attention scores, which could be non-informative and not contribute
to the value computation. We also observe that this phenomenon (at least partially)
stems from tokens’ inner dependence on attention scores as a result of softmax
normalization. After relaxing such dependence by replacing softmax attention
with other attention operations, such as sigmoid attention without normalization,
attention sinks do not emerge in LMs up to 1B parameters. The code is available
at https://github.com/sail-sg/Attention-Sink.



根据您提供的完整论文列表和技术落地情况，这是更新后的**预期成果及考核指标（中期报告）**：

## 预期成果及考核指标（中期报告）

1. 研究成果总结报告一份，涵盖医疗大模型知识编辑、知识图谱推理、药物推荐、序列推荐增强等核心技术方案与实验验证结果。
2. 完成3项核心技术原型验证：基于模型合并的医疗知识编辑技术（成功率96.95%，训练成本降低30%）；知识图谱推理增强系统（MRR提升5%，H@10提升7%）；统一模型融合框架（计算成本降低50%，性能提升10%）。
3. 构建医疗AI技术体系，涵盖知识动态更新、多中心药物推荐、长尾序列推荐等关键能力；完成原型系统测试，3项技术均有开源代码支撑。
4. 联合发表高质量国际学术论文14篇，其中已录用12篇（包括ACL 3篇、AAAI 3篇、TOIS 2篇、SIGIR、COLING、CIKM、FCS各1篇），AAAI 2026在投2篇；联合申请发明专利3-4项。
5. 联合培养博士研究生5名（Qidong Liu、Derong Xu、Yimin Deng、Zichuan Fu、Yejing Wang等），硕士研究生8-10名，其中6位以第一作者或共同第一作者身份在顶会发表论文，人才培养成效显著。