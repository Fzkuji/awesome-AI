## ğŸ“ é‡æ–°ç»„ç»‡çš„æ®µè½å»ºè®®

è¿™é‡Œæ˜¯ä¸€ä¸ªæ›´æ¸…æ™°çš„è¡¨è¿°ï¼š

---

To address this limitation, we propose two key innovations that work synergistically:

**First**, we introduce **Negative Value Masking (NVM)**, a variant of ReLU that operates in the log-domain:

$$\tilde{s}_i = \begin{cases} s_i & \text{if } s_i \geq 0 \ -\infty & \text{if } s_i < 0 \end{cases}$$

Unlike standard ReLU which maps negative values to zero, NVM maps them to negative infinity, effectively **eliminating** these positions entirely when combined with softmax. This hard masking ensures that positions with negative attention scores are completely removed from consideration rather than merely suppressed.

**Second**, we propose **Elastic-Softmax**, which combines NVM with learnable parameters to create an adaptive attention mechanism:

$$\text{ElasticSoftmax}(\mathbf{s})_i = \frac{\exp(\tilde{s}_i / \tau)}{\sum_{j=1}^{n} \exp(\tilde{s}_j / \tau) + \beta} \quad \text{for } i = 1, \ldots, n$$

where $\tilde{s}_i$ is the output of NVM, $\tau$ is a learnable temperature parameter controlling the sharpness of the distribution, and $\beta$ is a learnable offset parameter that prevents saturation and enables fine-grained control over the attention weights.

By combining these two components, Elastic-Softmax achieves both **hard sparsity** (through NVM) and **adaptive flexibility** (through learnable parameters), allowing the model to dynamically determine both which positions to attend to and how to distribute attention among them.

---

## ğŸ”„ æˆ–è€…æ›´ç®€æ´çš„ç‰ˆæœ¬ï¼š

---

To address this limitation, we propose combining two complementary techniques:

**Negative Value Masking (NVM)**: A ReLU variant for the log-domain that maps negative values to $-\infty$ instead of zero: $$\tilde{s}_i = \begin{cases} s_i & \text{if } s_i \geq 0 \ -\infty & \text{if } s_i < 0 \end{cases}$$

This ensures complete elimination of negative attention scores rather than soft suppression.

**Elastic-Softmax**: An adaptive attention mechanism that combines NVM with learnable parameters: $$\text{ElasticSoftmax}(\mathbf{s})_i = \frac{\exp(\tilde{s}_i / \tau)}{\sum_{j=1}^{n} \exp(\tilde{s}_j / \tau) + \beta}$$

where $\tau$ (temperature) and $\beta$ (offset) are learnable parameters that allow the model to adaptively control attention sharpness and prevent saturation.

Together, these innovations enable both hard sparsity and adaptive attention distribution.

---

å…³é”®æ”¹è¿›ï¼š

1. âœ… æ˜ç¡®æå‡ºæ˜¯**ä¸¤ä¸ªåˆ›æ–°**çš„ç»“åˆ
2. âœ… NVMä½œä¸ºç‹¬ç«‹è´¡çŒ®ï¼Œå¼ºè°ƒæ˜¯ReLUå˜ä½“
3. âœ… Elastic-Softmaxä½œä¸ºç»„åˆæ–¹æ¡ˆ
4. âœ… æ¸…æ¥šè¯´æ˜æ¯ä¸ªç»„ä»¶çš„ä½œç”¨


```python
def softmax_with_eps(self, x):
    x = np.maximum(x, -1e10)  # é™åˆ¶æœ€å°å€¼
    exp_x = np.exp(x)
    return exp_x / (np.sum(exp_x) + self.eps)
```


å“¦ï¼Œæˆ‘ç†è§£äº†ï¼ä½ æƒ³è¦çš„æ˜¯ï¼š**å¦‚æœåŸå§‹è¾“å…¥xå°äº0ï¼Œåˆ™å˜æˆ-infï¼›å¦åˆ™ä¿æŒåŸå€¼**ã€‚è¿™ç›¸å½“äºReLUçš„å˜ä½“ã€‚

## æ­£ç¡®çš„æ•°å­¦å…¬å¼


$$\text{output}_i = \frac{\exp(\text{ReLU}_{\infty}(x_i))}{\sum_{j} \exp(\text{ReLU}_{\infty}(x_j)) + \epsilon}$$

å…¶ä¸­ $$\text{ReLU}_{\infty}(x) = \begin{cases} x & \text{if } x \geq 0 \\ -\infty & \text{if } x < 0 \end{cases}$$

## ä»£ç å®ç°

```python
import numpy as np
import torch

def relu_inf_softmax(x, eps=1e-10):
    """
    å°†è´Ÿå€¼å˜ä¸º-infï¼Œéè´Ÿå€¼ä¿æŒä¸å˜ï¼Œç„¶ååº”ç”¨å¸¦epsilonçš„softmax
    """
    # Step 1: è´Ÿå€¼å˜-infï¼Œéè´Ÿå€¼ä¿æŒ
    masked_x = np.where(x >= 0, x, -np.inf)
    
    # Step 2: å¸¦epsilonçš„softmax
    exp_x = np.exp(masked_x)
    return exp_x / (np.sum(exp_x) + eps)

# PyTorchç‰ˆæœ¬
def relu_inf_softmax_torch(x, eps=1e-10):
    # è´Ÿå€¼å˜-inf
    masked_x = torch.where(x >= 0, x, float('-inf'))
    
    # Softmax with epsilon
    exp_x = torch.exp(masked_x)
    return exp_x / (torch.sum(exp_x) + eps)

# æ•°å€¼ç¨³å®šç‰ˆæœ¬
def stable_relu_inf_softmax(x, eps=1e-10):
    """
    æ•°å€¼ç¨³å®šç‰ˆæœ¬ï¼Œå¤„ç†å…¨è´Ÿå€¼æƒ…å†µ
    """
    mask = x >= 0
    
    # æ£€æŸ¥æ˜¯å¦å…¨éƒ¨ä¸ºè´Ÿå€¼
    if not np.any(mask):
        # å…¨è´Ÿå€¼æ—¶è¿”å›å‡åŒ€åˆ†å¸ƒï¼ˆæˆ–å…¨é›¶ï¼Œå–å†³äºéœ€æ±‚ï¼‰
        return np.ones_like(x) / len(x)
    
    # è´Ÿå€¼å˜-inf
    masked_x = np.where(mask, x, -np.inf)
    
    # æ•°å€¼ç¨³å®šçš„softmaxï¼ˆå‡å»æœ€å¤§å€¼ï¼‰
    max_val = np.max(masked_x[mask])
    exp_x = np.exp(masked_x - max_val)
    
    return exp_x / (np.sum(exp_x) + eps)
```

## ç¤ºä¾‹

```python
x = np.array([-2.0, 3.0, -1.0, 5.0, 0.0, 2.0])

result = relu_inf_softmax(x)
print(f"è¾“å…¥: {x}")
print(f"æ©ç å: {np.where(x >= 0, x, -np.inf)}")
print(f"è¾“å‡º: {result}")
print(f"å’Œ: {np.sum(result)}")

# è¾“å…¥: [-2.  3. -1.  5.  0.  2.]
# æ©ç å: [-inf   3. -inf   5.   0.   2
```