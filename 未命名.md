

为了更好地理解 LLM 的记忆机制，我们首先将这些类别与人类记忆系统进行类比映射。我们依据信息压缩处理的程度，同样将记忆分成三类：

- **感觉记忆**（Sensory Memory）则体现为人类感官的原始输入，对应于 LLM 的 RAG 方法，例如 MemoChat 和 Recursively Summarizing 通过检索或生成外部备忘录增强对话能力。这种映射不仅帮助我们理解 LLM 记忆设计的灵感来源，还揭示了其在模拟人类认知方面的潜力与局限。
    
- **工作记忆**（Working/Short-term Memory）是人类在短时间内处理和操作信息的临时存储空间，类似于 LLM 的隐藏状态（如 Transformer 的注意力机制或循环网络的状态），用于维持当前任务的上下文，例如 Controllable Working Memory 研究中控制模型的短期信息优先级。
    
- **长期记忆**（Long-term Memory）存储于大脑神经网络中，包含事实、经验和技能，类似于 LLM 通过训练更新模型参数来固化知识的过程，例如 MemoryBank 通过持续学习适应用户个性化的需求。


1 2 3 4 5 6 ...


1 2 3(:)     4.1(感) 5.1(觉) ..
	    4.2(工) 5.2(作) ...
			  5.3(长) ...

q1 q2 q3 q4

q1 q2 q3 q4.1(theta4-1)
		q4.2(theta4-2)

k1 k2 k3 k4

k1 k2 k3 q4.1(theta4-1)
		q4.2(theta4-2)



Your name: Zichuan FU
Conf: ICLR 26
Track: Main
Paper ID: 4365
New or Old?: Old
Title: Attention Needs to Focus: Lazy Attention to Suppress Interference and Filter Irrelevance
Author list: Zichuan Fu, Wentao Song, Xian Wu, Guojing Li, Yejing Wang, Yingying Zhang, Yimin Deng, Derong Xu, Hanyu Yan, Jiaxuan Li, Yefeng Zheng, Xiangyu Zhao
Outline link (for old papers, please include the comments of last submission): https://hackmd.io/@Zyp1vgMzSHKpqdQGrTY4Iw/S1_8Upmakg/edit
Overleaf link (for old papers, please change your project title accordingly): https://www.overleaf.com/1893319855skdgbspqhnhk#37a029
Corresponding author(s): Xiangyu Zhao


```
# -*- coding: utf-8 -*-  
# Copyright (c) 2023-2025, Songlin Yang, Yu Zhang  
  
from __future__ import annotations  
  
import math  
import warnings  
from typing import TYPE_CHECKING, Optional, Tuple  
  
import torch  
import torch.nn as nn  
import torch.nn.functional as F  
from einops import rearrange  
from transformers.utils import logging  
  
  
def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:  
    """  
    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)    """    batch, num_key_value_heads, slen, head_dim = hidden_states.shape  
    if n_rep == 1:  
        return hidden_states  
    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)  
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)  
  
  
from fla.layers.utils import pad_input, unpad_input  
from fla.modules import RMSNorm, RotaryEmbedding  
from fla.ops.utils.index import prepare_lens_from_mask  
  
if TYPE_CHECKING:  
    from fla.models.utils import Cache  
  
try:  
    from flash_attn import flash_attn_func, flash_attn_varlen_func  
except ImportError:  
    warnings.warn(  
        "Flash Attention is not installed. Please install it via `pip install flash-attn --no-build-isolation`",  
        category=ImportWarning  
    )  
    flash_attn_func = None  
  
logger = logging.get_logger(__name__)  
  
  
class SWAttention(nn.Module):  
  
    def __init__(  
            self,  
            hidden_size: int = 2048,  
            num_heads: int = 32,  
            num_kv_heads: Optional[int] = None,  
            qkv_bias: bool = False,  
            qk_norm: bool = False,  
            window_size: Optional[int] = None,  
            rope_theta: Optional[float] = 10000.,  
            max_position_embeddings: Optional[int] = None,  
            layer_idx: int = None,  
            use_learnable_bias: bool = True,  # 是否使用可学习的位置bias  
            max_bias_length: int = 1024,  # bias矩阵的最大尺寸  
    ):  
        super().__init__()  
  
        self.hidden_size = hidden_size  
        self.num_heads = num_heads  
        if num_kv_heads is None:  
            self.num_kv_heads = self.num_heads  
        else:  
            self.num_kv_heads = num_kv_heads  
        self.num_kv_groups = num_heads // self.num_kv_heads  
        self.head_dim = self.hidden_size // self.num_heads  
        self.kv_dim = self.num_kv_heads * self.head_dim  
        self.scaling = self.head_dim ** -0.5  
        self.qkv_bias = qkv_bias  
        self.qk_norm = qk_norm  
  
        self.window_size = window_size  
        self.rope_theta = rope_theta  
        self.max_position_embeddings = max_position_embeddings  
        self.layer_idx = layer_idx  
        self.use_learnable_bias = use_learnable_bias  
        self.max_bias_length = max_bias_length  
  
        # No longer requiring flash attention  
        # Don't pre-allocate mask to save memory  
        self.q_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=self.qkv_bias)  
        self.k_proj = nn.Linear(self.hidden_size, self.kv_dim, bias=self.qkv_bias)  
        self.v_proj = nn.Linear(self.hidden_size, self.kv_dim, bias=self.qkv_bias)  
        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)  
  
        # 初始化可学习的位置bias矩阵  
        if self.use_learnable_bias:  
            self._init_learnable_position_bias()  
  
        # 每个头的softmax归一化偏置（加到分母上）  
        # 初始化为0，这样初始时softmax行为正常（和为1）  
        self.softmax_norm_bias = nn.Parameter(torch.zeros(self.num_heads))  
  
    def _init_learnable_position_bias(self):  
        """初始化可学习的位置bias参数，每个头都有独立的对角线参数（仅用于causal attention）"""  
        # 创建可学习的对角线参数：[num_heads, max_bias_length]  
        # 存储主对角线和所有下对角线的值  
        self.learnable_bias_diagonals = nn.Parameter(  
            torch.zeros(self.num_heads, self.max_bias_length)  
        )  
  
        # 用很小的随机值初始化 (例如正态分布, std=1e-3)  
        nn.init.normal_(self.learnable_bias_diagonals, mean=0.0, std=1e-3)  
        # # 使用ALiBi初始化  
        # self._init_with_alibi_diagonals()  
  
    def get_learnable_bias(self):  
        """获取可学习的对角线bias参数"""  
        # 直接返回对角线参数  
        # [num_heads, max_bias_length]  
        return self.learnable_bias_diagonals  
  
    def apply_learnable_bias_efficient(self, attn_weights):  
        """高效地应用对角线 bias，使用GPU并行操作，避免大内存占用"""  
        batch_size, num_heads, seq_len_q, seq_len_k = attn_weights.shape  
  
        # 创建相对位置索引矩阵 [seq_len_q, seq_len_k]        q_pos = torch.arange(seq_len_q, device=attn_weights.device, dtype=torch.long)  
        k_pos = torch.arange(seq_len_k, device=attn_weights.device, dtype=torch.long)  
        rel_pos = q_pos.unsqueeze(1) - k_pos.unsqueeze(0)  
  
        # Causal mask: 只处理下三角部分  
        causal_mask = (rel_pos >= 0).to(attn_weights.dtype)  
  
        # 限制相对位置在参数范围内  
        rel_pos = torch.clamp(rel_pos, min=0, max=self.max_bias_length - 1)  
  
        # 直接使用高级索引从learnable_bias_diagonals中获取对应的bias值  
        # learnable_bias_diagonals: [num_heads, max_bias_length]  
        # rel_pos: [seq_len_q, seq_len_k]        # 结果: [num_heads, seq_len_q, seq_len_k]  
        bias_values = self.learnable_bias_diagonals[:, rel_pos]  
  
        # 应用causal mask（上三角部分设为0）  
        bias_values = bias_values * causal_mask  
  
        # 扩展到batch维度并相加  
        # [num_heads, seq_len_q, seq_len_k] -> [1, num_heads, seq_len_q, seq_len_k] -> [batch_size, num_heads, seq_len_q, seq_len_k]  
        bias_values = bias_values.unsqueeze(0).expand(batch_size, -1, -1, -1)  
  
        return attn_weights + bias_values  
  
    def forward(  
            self,  
            hidden_states: torch.Tensor,  
            attention_mask: Optional[torch.LongTensor] = None,  
            past_key_values: Optional[Cache] = None,  
            output_attentions: bool = True,  
            use_cache: bool = False,  
            **kwargs,  
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:  
  
        batch_size, q_len, _ = hidden_states.size()  
  
        q = rearrange(self.q_proj(hidden_states), '... (h d) -> ... h d', d=self.head_dim)  
        k = rearrange(self.k_proj(hidden_states), '... (h d) -> ... h d', d=self.head_dim)  
        v = rearrange(self.v_proj(hidden_states), '... (h d) -> ... h d', d=self.head_dim)  
  
        if self.qk_norm:  
            q, k = self.q_norm(q), self.k_norm(k)  
  
        # Regular attention implementation (no flash attention)  
        # Reshape for attention computation: (B, T, num_heads, head_dim) -> (B, num_heads, T, head_dim)        q = q.transpose(1, 2)  
        k = k.transpose(1, 2)  
        v = v.transpose(1, 2)  
  
        # Handle grouped multi-query attention using repeat_kv  
        k = repeat_kv(k, self.num_kv_groups)  
        v = repeat_kv(v, self.num_kv_groups)  
  
        # Compute attention scores  
        attn_scores = torch.matmul(q, k.transpose(2, 3)) * self.scaling  
  
        # 应用可学习的位置bias  
        if self.use_learnable_bias:  
            attn_scores = self.apply_learnable_bias_efficient(attn_scores)  
  
        # Apply attention mask if provided  
        if attention_mask is not None:  
            causal_mask = attention_mask[:, :, :, :k.shape[-2]]  
            attn_scores = attn_scores + causal_mask  
  
        # 小于0的score设置成-inf (使用可微分的方式)  
        # 使用大的负值代替-inf，保持梯度流动  
        attn_scores = torch.where(attn_scores < 0, attn_scores - 1e9, attn_scores)  
  
        # Apply softmax with normalization bias for each head  
        # 计算exp(scores)  
        attn_scores_exp = torch.exp(attn_scores - attn_scores.max(dim=-1, keepdim=True)[0])  # 数值稳定性  
  
        # 添加归一化偏置到分母  
        # self.softmax_norm_bias 形状: [num_heads]  
        # 扩展为: [1, num_heads, 1, 1] 以匹配 attn_scores_exp 的形状 [batch, num_heads, seq_q, seq_k]        norm_bias = self.softmax_norm_bias.view(1, self.num_heads, 1, 1)  
        denominator = attn_scores_exp.sum(dim=-1, keepdim=True) + torch.exp(norm_bias)  # 分母加上exp(bias)  
  
        # 计算归一化的attention weights  
        attn_weights = (attn_scores_exp / denominator).to(q.dtype)  
  
        attentions = attn_weights  
  
        # Apply attention to values  
        attn_output = torch.matmul(attn_weights, v)  
  
        # Reshape back: (B, num_heads, T, head_dim) -> (B, T, num_heads, head_dim) -> (B, T, hidden_size)  
        attn_output = attn_output.transpose(1, 2).contiguous()  
        attn_output = attn_output.reshape(batch_size, q_len, -1)  
        o = self.o_proj(attn_output)  
  
        return o, attentions, past_key_values
```




```
# -*- coding: utf-8 -*-  
# Copyright (c) 2023-2025, Songlin Yang, Yu Zhang  
  
from __future__ import annotations  
  
import math  
import warnings  
from typing import TYPE_CHECKING, Optional, Tuple  
  
import torch  
import torch.nn as nn  
import torch.nn.functional as F  
from einops import rearrange  
from transformers.utils import logging  
  
# Triton imports  
try:  
    import triton  
    import triton.language as tl  
    HAS_TRITON = True  
except ImportError:  
    HAS_TRITON = False  
    warnings.warn("Triton is not installed. Please install it via `pip install triton`")  
  
def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:  
    batch, num_key_value_heads, slen, head_dim = hidden_states.shape  
    if n_rep == 1:  
        return hidden_states  
    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)  
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)  
  
from fla.layers.utils import pad_input, unpad_input  
from fla.modules import RMSNorm, RotaryEmbedding  
from fla.ops.utils.index import prepare_lens_from_mask  
  
if TYPE_CHECKING:  
    from fla.models.utils import Cache  
  
logger = logging.get_logger(__name__)  
  
  
# Triton Kernels for SWAttention  
if HAS_TRITON:  
    @triton.jit  
    def _swattn_fwd_kernel(  
        Q, K, V,  
        learnable_bias_diagonals,  
        softmax_offset,  
        O, L, M,  
        s_qz, s_qh, s_qt, s_qd,  
        s_kz, s_kh, s_kt, s_kd,  
        s_vz, s_vh, s_vt, s_vd,  
        s_oz, s_oh, s_ot, s_od,  
        batch_size, num_heads, seq_len,  
        SM_SCALE: tl.constexpr,  
        BLOCK_M: tl.constexpr,  
        BLOCK_N: tl.constexpr,  
        HEAD_DIM: tl.constexpr,  
        MAX_BIAS_LENGTH: tl.constexpr,  
        USE_LEARNABLE_BIAS: tl.constexpr,  
    ):  
        pid_m = tl.program_id(0)  
        pid_bh = tl.program_id(1)  
        pid_b = pid_bh // num_heads  
        pid_h = pid_bh % num_heads  
  
        q_offset = pid_b * s_qz + pid_h * s_qh  
        k_offset = pid_b * s_kz + pid_h * s_kh  
        v_offset = pid_b * s_vz + pid_h * s_vh  
        o_offset = pid_b * s_oz + pid_h * s_oh  
  
        offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)  
        offs_d = tl.arange(0, HEAD_DIM)  
        q_ptrs = Q + q_offset + (offs_m[:, None] * s_qt + offs_d[None, :] * s_qd)  
  
        acc = tl.zeros([BLOCK_M, HEAD_DIM], dtype=tl.float32)  
        l_i = tl.zeros([BLOCK_M], dtype=tl.float32)  
        m_i = tl.full([BLOCK_M], value=float('-inf'), dtype=tl.float32)  
          
        mask_m = offs_m < seq_len  
        q = tl.load(q_ptrs, mask=mask_m[:, None], other=0.0)  
  
        # Pass 1: Compute Softmax Denominator  
        for start_n in range(0, seq_len, BLOCK_N):  
            offs_n = start_n + tl.arange(0, BLOCK_N)  
            mask_n = offs_n < seq_len  
            k_ptrs = K + k_offset + (offs_n[None, :] * s_kt + offs_d[:, None] * s_kd)  
            k = tl.load(k_ptrs, mask=mask_n[None, :], other=0.0)  
  
            s_ij = tl.dot(q, k.T) * SM_SCALE  
            causal_mask = offs_m[:, None] >= offs_n[None, :]  
            s_ij = tl.where(causal_mask, s_ij, float('-inf'))  
  
            if USE_LEARNABLE_BIAS:  
                rel_pos = offs_m[:, None] - offs_n[None, :]  
                valid_mask = (rel_pos >= 0) & (rel_pos < MAX_BIAS_LENGTH)  
                indices = tl.where(valid_mask, rel_pos, 0)  
                bias_ptr = learnable_bias_diagonals + pid_h * MAX_BIAS_LENGTH  
                bias_val = tl.load(bias_ptr + indices, mask=valid_mask, other=0.0)  
                s_ij += bias_val  
  
            m_ij = tl.max(s_ij, axis=1)  
            m_i_new = tl.maximum(m_i, m_ij)  
            p_ij = tl.exp(s_ij - m_i_new[:, None])  
            alpha = tl.exp(m_i - m_i_new)  
            l_i = l_i * alpha + tl.sum(p_ij, axis=1)  
            m_i = m_i_new  
  
        l_i_inv = 1.0 / l_i  
          
        # Pass 2: Compute Final Output  
        for start_n in range(0, seq_len, BLOCK_N):  
            offs_n = start_n + tl.arange(0, BLOCK_N)  
            mask_n = offs_n < seq_len  
            k_ptrs = K + k_offset + (offs_n[None, :] * s_kt + offs_d[:, None] * s_kd)  
            v_ptrs = V + v_offset + (offs_n[:, None] * s_vt + offs_d[None, :] * s_vd)  
            k = tl.load(k_ptrs, mask=mask_n[None, :], other=0.0)  
            v = tl.load(v_ptrs, mask=mask_n[:, None], other=0.0)  
  
            s_ij = tl.dot(q, k.T) * SM_SCALE  
            causal_mask = offs_m[:, None] >= offs_n[None, :]  
            s_ij = tl.where(causal_mask, s_ij, float('-inf'))  
  
            if USE_LEARNABLE_BIAS:  
                rel_pos = offs_m[:, None] - offs_n[None, :]  
                valid_mask = (rel_pos >= 0) & (rel_pos < MAX_BIAS_LENGTH)  
                indices = tl.where(valid_mask, rel_pos, 0)  
                bias_ptr = learnable_bias_diagonals + pid_h * MAX_BIAS_LENGTH  
                bias_val = tl.load(bias_ptr + indices, mask=valid_mask, other=0.0)  
                s_ij += bias_val  
  
            p_ij = tl.exp(s_ij - m_i[:, None]) * l_i_inv[:, None]  
              
            offset_h = tl.load(softmax_offset + pid_h)  
            offset_val = tl.abs(offset_h + 1.0)  
            num_visible = (offs_n[None, :] + 1.0).to(tl.float32)  
            adaptive_offset = offset_val / num_visible  
            p_ij = p_ij - adaptive_offset  
            p_ij = tl.where(p_ij > 0, p_ij, 0.0)  
  
            acc += tl.dot(p_ij.to(Q.dtype.element_ty), v)  
  
        o_ptrs = O + o_offset + (offs_m[:, None] * s_ot + offs_d[None, :] * s_od)  
        tl.store(o_ptrs, acc, mask=mask_m[:, None])  
          
        L_ptrs = L + pid_bh * seq_len + offs_m  
        M_ptrs = M + pid_bh * seq_len + offs_m  
        tl.store(L_ptrs, l_i, mask=mask_m)  
        tl.store(M_ptrs, m_i, mask=mask_m)  
  
    @triton.jit  
    def _swattn_bwd_kernel(  
        Q, K, V, O, dO,  
        learnable_bias_diagonals, softmax_offset,  
        L, M,  
        dQ, dK, dV,  
        d_learnable_bias, d_softmax_offset,  
        s_qz, s_qh, s_qt, s_qd,  
        s_kz, s_kh, s_kt, s_kd,  
        s_vz, s_vh, s_vt, s_vd,  
        batch_size, num_heads, seq_len,  
        SM_SCALE: tl.constexpr,  
        BLOCK_M: tl.constexpr,  
        BLOCK_N: tl.constexpr,  
        HEAD_DIM: tl.constexpr,  
        MAX_BIAS_LENGTH: tl.constexpr,  
        USE_LEARNABLE_BIAS: tl.constexpr,  
    ):  
        pid_m = tl.program_id(0) # Parallelize over M  
        pid_bh = tl.program_id(1)  
        pid_b = pid_bh // num_heads  
        pid_h = pid_bh % num_heads  
  
        # Pointers  
        q_offset = pid_b * s_qz + pid_h * s_qh  
        k_offset = pid_b * s_kz + pid_h * s_kh  
        v_offset = pid_b * s_vz + pid_h * s_vh  
          
        offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)  
        offs_d = tl.arange(0, HEAD_DIM)  
          
        # Load Q, dO for the current block  
        mask_m = offs_m < seq_len  
        q_ptrs = Q + q_offset + (offs_m[:, None] * s_qt + offs_d[None, :] * s_qd)  
        q = tl.load(q_ptrs, mask=mask_m[:, None], other=0.0)  
          
        do_ptrs = dO + q_offset + (offs_m[:, None] * s_qt + offs_d[None, :] * s_qd)  
        do = tl.load(do_ptrs, mask=mask_m[:, None], other=0.0)  
  
        # Load L, M  
        l_ptrs = L + pid_bh * seq_len + offs_m  
        m_ptrs = M + pid_bh * seq_len + offs_m  
        l_i = tl.load(l_ptrs, mask=mask_m, other=1.0)  
        m_i = tl.load(m_ptrs, mask=mask_m, other=float('-inf'))  
  
        # Initialize dQ accumulator  
        dq_acc = tl.zeros([BLOCK_M, HEAD_DIM], dtype=tl.float32)  
  
        # Loop over K, V blocks  
        for start_n in range(0, seq_len, BLOCK_N):  
            offs_n = start_n + tl.arange(0, BLOCK_N)  
            mask_n = offs_n < seq_len  
  
            # Load K, V  
            k_ptrs = K + k_offset + (offs_n[None, :] * s_kt + offs_d[:, None] * s_kd)  
            v_ptrs = V + v_offset + (offs_n[:, None] * s_vt + offs_d[None, :] * s_vd)  
            k = tl.load(k_ptrs, mask=mask_n[None, :], other=0.0)  
            v = tl.load(v_ptrs, mask=mask_n[:, None], other=0.0)  
  
            # Recompute S_ij  
            s_ij = tl.dot(q, k.T) * SM_SCALE  
            causal_mask = offs_m[:, None] >= offs_n[None, :]  
            s_ij = tl.where(causal_mask, s_ij, float('-inf'))  
  
            if USE_LEARNABLE_BIAS:  
                rel_pos = offs_m[:, None] - offs_n[None, :]  
                valid_mask = (rel_pos >= 0) & (rel_pos < MAX_BIAS_LENGTH)  
                indices = tl.where(valid_mask, rel_pos, 0)  
                bias_ptr = learnable_bias_diagonals + pid_h * MAX_BIAS_LENGTH  
                bias_val = tl.load(bias_ptr + indices, mask=valid_mask, other=0.0)  
                s_ij += bias_val  
              
            # Recompute P_ij  
            p_raw = tl.exp(s_ij - m_i[:, None]) / l_i[:, None]  
              
            offset_h = tl.load(softmax_offset + pid_h)  
            offset_val = tl.abs(offset_h + 1.0)  
            num_visible = (offs_n[None, :] + 1.0).to(tl.float32)  
            adaptive_offset = offset_val / num_visible  
            p_final = p_raw - adaptive_offset  
            p_final = tl.where(p_final > 0, p_final, 0.0)  
  
            # Compute dP  
            dp = tl.dot(do, v.T)  
            relu_mask = p_final > 0  
            dp_raw = dp * relu_mask.to(dp.dtype)  
  
            # Compute dS  
            ds = p_raw * (dp_raw - tl.sum(dp_raw * p_raw, axis=1)[:, None])  
            ds = ds * SM_SCALE  
            ds = ds.to(Q.dtype.element_ty)  
  
            # Accumulate dQ  
            dq_acc += tl.dot(ds, k)  
  
            # Accumulate dK, dV with atomic adds  
            dk_ptrs = dK + k_offset + (offs_n[None, :] * s_kt + offs_d[:, None] * s_kd)  
            dv_ptrs = dV + v_offset + (offs_n[:, None] * s_vt + offs_d[None, :] * s_vd)  
              
            ds_t = tl.trans(ds)  
            tl.atomic_add(dk_ptrs, tl.dot(ds_t, q))  
            tl.atomic_add(dv_ptrs, tl.dot(p_final.to(V.dtype.element_ty).T, do))  
  
            # Accumulate d_learnable_bias, d_softmax_offset  
            if USE_LEARNABLE_BIAS:  
                rel_pos = offs_m[:, None] - offs_n[None, :]  
                valid_mask = (rel_pos >= 0) & (rel_pos < MAX_BIAS_LENGTH)  
                indices = tl.where(valid_mask, rel_pos, 0)  
                d_bias_ptr = d_learnable_bias + pid_h * MAX_BIAS_LENGTH  
                value_to_add = (ds * valid_mask.to(ds.dtype)).to(d_learnable_bias.dtype.element_ty)  
                tl.atomic_add(d_bias_ptr + indices, value_to_add)  
  
            d_adaptive_offset = -dp_raw  
            d_offset_val = tl.sum(d_adaptive_offset / num_visible)  
            d_offset_h = d_offset_val * tl.where(offset_h + 1.0 > 0, 1.0, -1.0)  
            tl.atomic_add(d_softmax_offset + pid_h, d_offset_h.to(d_softmax_offset.dtype.element_ty))  
  
        # Write back dQ  
        dq_ptrs = dQ + q_offset + (offs_m[:, None] * s_qt + offs_d[None, :] * s_qd)  
        tl.store(dq_ptrs, dq_acc, mask=mask_m[:, None])  
  
  
class SWAttentionTritonFunction(torch.autograd.Function):  
    @staticmethod  
    def forward(ctx, q, k, v, learnable_bias, softmax_offset, scaling):  
        batch_size, num_heads, seq_len, head_dim = q.shape  
          
        if head_dim not in [16, 32, 64, 128]:  
            raise ValueError(f"Triton kernel only supports head dimensions of 16, 32, 64, or 128, but got {head_dim}")  
  
        o = torch.empty_like(q)  
        l = torch.empty((batch_size * num_heads, seq_len), device=q.device, dtype=torch.float32)  
        m = torch.empty((batch_size * num_heads, seq_len), device=q.device, dtype=torch.float32)  
  
        BLOCK_M = 128  
        BLOCK_N = 64  
        grid = (triton.cdiv(seq_len, BLOCK_M), batch_size * num_heads)  
  
        use_learnable_bias = learnable_bias is not None  
        max_bias_length = learnable_bias.shape[1] if use_learnable_bias else 0  
  
        _swattn_fwd_kernel[grid](  
            q, k, v,  
            learnable_bias, softmax_offset,  
            o, l, m,  
            q.stride(0), q.stride(1), q.stride(2), q.stride(3),  
            k.stride(0), k.stride(1), k.stride(2), k.stride(3),  
            v.stride(0), v.stride(1), v.stride(2), v.stride(3),  
            o.stride(0), o.stride(1), o.stride(2), o.stride(3),  
            batch_size, num_heads, seq_len,  
            SM_SCALE=scaling,  
            BLOCK_M=BLOCK_M,  
            BLOCK_N=BLOCK_N,  
            HEAD_DIM=head_dim,  
            MAX_BIAS_LENGTH=max_bias_length,  
            USE_LEARNABLE_BIAS=use_learnable_bias,  
            num_warps=4,  
        )  
          
        ctx.save_for_backward(q, k, v, o, l, m, learnable_bias, softmax_offset)  
        ctx.scaling = scaling  
        ctx.max_bias_length = max_bias_length  
        ctx.use_learnable_bias = use_learnable_bias  
        return o  
  
    @staticmethod  
    def backward(ctx, do):  
        q, k, v, o, l, m, learnable_bias, softmax_offset = ctx.saved_tensors  
        scaling = ctx.scaling  
        max_bias_length = ctx.max_bias_length  
        use_learnable_bias = ctx.use_learnable_bias  
  
        batch_size, num_heads, seq_len, head_dim = q.shape  
  
        dq = torch.empty_like(q)  
        dk = torch.zeros_like(k)  
        dv = torch.zeros_like(v)  
          
        d_learnable_bias = torch.zeros_like(learnable_bias) if use_learnable_bias else None  
        d_softmax_offset = torch.zeros_like(softmax_offset)  
  
        BLOCK_M = 128  
        BLOCK_N = 64  
        grid = (triton.cdiv(seq_len, BLOCK_M), batch_size * num_heads)  
  
        _swattn_bwd_kernel[grid](  
            q, k, v, o, do,  
            learnable_bias, softmax_offset,  
            l, m,  
            dq, dk, dv,  
            d_learnable_bias, d_softmax_offset,  
            q.stride(0), q.stride(1), q.stride(2), q.stride(3),  
            k.stride(0), k.stride(1), k.stride(2), k.stride(3),  
            v.stride(0), v.stride(1), v.stride(2), v.stride(3),  
            batch_size, num_heads, seq_len,  
            SM_SCALE=scaling,  
            BLOCK_M=BLOCK_M,  
            BLOCK_N=BLOCK_N,  
            HEAD_DIM=head_dim,  
            MAX_BIAS_LENGTH=max_bias_length,  
            USE_LEARNABLE_BIAS=use_learnable_bias,  
            num_warps=4,  
        )  
        return dq, dk, dv, d_learnable_bias, d_softmax_offset, None  
  
  
class SWAttention(nn.Module):  
    def __init__(  
            self,  
            hidden_size: int = 2048,  
            num_heads: int = 32,  
            num_kv_heads: Optional[int] = None,  
            qkv_bias: bool = False,  
            qk_norm: bool = False,  
            window_size: Optional[int] = None,  
            rope_theta: Optional[float] = 10000.,  
            max_position_embeddings: Optional[int] = None,  
            layer_idx: int = None,  
            use_learnable_bias: bool = True,  
            max_bias_length: int = 1024,  
    ):  
        super().__init__()  
  
        self.hidden_size = hidden_size  
        self.num_heads = num_heads  
        self.num_kv_heads = num_kv_heads if num_kv_heads is not None else num_heads  
        self.num_kv_groups = self.num_heads // self.num_kv_heads  
        self.head_dim = self.hidden_size // self.num_heads  
        self.kv_dim = self.num_kv_heads * self.head_dim  
        self.scaling = self.head_dim ** -0.5  
        self.qkv_bias = qkv_bias  
        self.qk_norm = qk_norm  
        self.window_size = window_size  
          
        self.rope_theta = rope_theta  
        self.layer_idx = layer_idx  
        self.use_learnable_bias = use_learnable_bias  
        self.max_bias_length = max_bias_length  
  
        self.q_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=self.qkv_bias)  
        self.k_proj = nn.Linear(self.hidden_size, self.kv_dim, bias=self.qkv_bias)  
        self.v_proj = nn.Linear(self.hidden_size, self.kv_dim, bias=self.qkv_bias)  
        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)  
  
        if self.use_learnable_bias:  
            self.learnable_bias_diagonals = nn.Parameter(  
                torch.zeros(self.num_heads, self.max_bias_length)  
            )  
            nn.init.normal_(self.learnable_bias_diagonals, mean=0.0, std=1e-3)  
        else:  
            self.register_parameter('learnable_bias_diagonals', None)  
  
        self.softmax_offset = nn.Parameter(torch.full((self.num_heads,), 0.0))  
        self.rotary = RotaryEmbedding(dim=self.head_dim, base=self.rope_theta)  
  
    def forward(  
            self,  
            hidden_states: torch.Tensor,  
            attention_mask: Optional[torch.LongTensor] = None,  
            past_key_values: Optional[Cache] = None,  
            output_attentions: bool = False,  
            use_cache: bool = False,  
            **kwargs,  
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:  
  
        if not HAS_TRITON:  
            raise ImportError("Triton is not installed, but is required for this SWAttention implementation.")  
        if past_key_values is not None or use_cache:  
            warnings.warn("Triton implementation of SWAttention does not currently support past_key_values (caching).")  
        if attention_mask is not None and (attention_mask.ndim > 2 or (attention_mask == 0).any()):  
             warnings.warn("Triton implementation of SWAttention only supports causal masking (implicit) and does not support padding masks.")  
  
        batch_size, q_len, _ = hidden_states.size()  
  
        q = rearrange(self.q_proj(hidden_states), 'b l (h d) -> b l h d', h=self.num_heads)  
        k = rearrange(self.k_proj(hidden_states), 'b l (h d) -> b l h d', h=self.num_kv_heads)  
        v = rearrange(self.v_proj(hidden_states), 'b l (h d) -> b l h d', h=self.num_kv_heads)  
  
        seqlen_offset = 0  
        if past_key_values is not None:  
            seqlen_offset = past_key_values.get_seq_length(self.layer_idx)  
  
        q, k = self.rotary(q, k, seqlen_offset=seqlen_offset)  
  
        q = rearrange(q, 'b l h d -> b h l d')  
        k = rearrange(k, 'b l h d -> b h l d')  
        v = rearrange(v, 'b l h d -> b h l d')  
  
        k = repeat_kv(k, self.num_kv_groups)  
        v = repeat_kv(v, self.num_kv_groups)  
  
        attn_output = SWAttentionTritonFunction.apply(  
            q, k, v,  
            self.learnable_bias_diagonals,  
            self.softmax_offset,  
            self.scaling  
        )  
  
        attn_output = rearrange(attn_output, 'b h l d -> b l (h d)')  
        o = self.o_proj(attn_output)  
  
        if output_attentions:  
            warnings.warn("`output_attentions=True` is not supported with the Triton kernel. Returning `None` for attentions.")  
  
        return o, None, past_key_values
```