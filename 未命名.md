Auto-regressive Language Models (LMs) assign significant attention to the first
token, even if it is not semantically important, which is known as attention sink.
This phenomenon has been widely adopted in applications such as streaming/long
context generation, KV cache optimization, inference acceleration, model
quantization, and others. Despite its widespread use, a deep understanding of
attention sink in LMs is still lacking. In this work, we first demonstrate that
attention sinks exist universally in auto-regressive LMs with various inputs, even
in small models. Furthermore, attention sink is observed to emerge during the
LM pre-training, motivating us to investigate how optimization, data distribution,
loss function, and model architecture in LM pre-training influence its emergence.
We highlight that attention sink emerges after effective optimization on sufficient
training data. The sink position is highly correlated with the loss function and data
distribution. Most importantly, we find that attention sink acts more like key biases,storing extra attention scores, which could be non-informative and not contribute
to the value computation. We also observe that this phenomenon (at least partially)
stems from tokens’ inner dependence on attention scores as a result of softmax
normalization. After relaxing such dependence by replacing softmax attention
with other attention operations, such as sigmoid attention without normalization,
attention sinks do not emerge in LMs up to 1B parameters. The code is available
at https://github.com/sail-sg/Attention-Sink.



根据图片内容，以下是markdown格式的表格：

## 表1. 技术落地/储备

|技术名称|应用阶段|技术指标|应用价值|可共享代码|
|---|---|---|---|---|
|例："基于x方法的x算法"。|例：在x数据集上测试/上线x系统...|已应用的填"宣化的"经济价值或...|内部共享...||

## 表2. 专利申请

|专利名称|腾讯编号或专利局申请号|申请人（申请单位）|发明人|
|---|---|---|---|
|由腾讯提交的专利，填写"pate..."，用"，"分隔全部申请单...|按顺序填写全部发明人，用"，"分隔...|||

## 表3. 联合论文发表（会议/期刊目录 km.woa.com/posts/show/552865）

| 题目                                                                                         | 状态  | 会议/期刊名称    | 年份   | 学术作者                                                      | 腾讯作者                                            |
| ------------------------------------------------------------------------------------------ | --- | ---------- | ---- | --------------------------------------------------------- | ----------------------------------------------- |
| When MOE Meets LLMs: Parameter Efficient Fine-tuning for Multi-task Medical Applications   | 已录用 | SIGIR 2024 | 2024 | Qidong Liu, Xiangyu Zhao, Yuanshao Zhu, Derong Xu         | Xian Wu, Yefeng Zheng                           |
| Multi-perspective Improvement of Knowledge Graph Completion with Large Language Models     | 已录用 | COLING     | 2024 | Derong Xu, Zhihong Zhu, Tong Xu, Xiangyu Zhao             | Ziheng Zhang, Zhenxi Lin, Xian Wu, Yefeng Zheng |
| Large Language Models for Generative Information Extraction                                | 已投稿 | ACL        | 2024 | Derong Xu, Wei Chen, Wenjun Peng, Chao Zhang              | Xian Wu, Yefeng Zheng                           |
| Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models         | 已录用 | CIKM       | 2024 | Derong Xu, Zhihong Zhu, Qidong Liu, Tong Xu, Xiangyu Zhao | Ziheng Zhang, Zhenxi Lin, Xian Wu, Yefeng Zheng |
| A Contrastive Pretrain Model with Prompt Tuning for Multi-center Medication Recommendation | 已录用 | TOIS       | 2024 | Qidong Liu, Xiangyu Zhao, Zijian Zhang, Tong Xu           | Zhaopeng Qiu, Xian Wu                           |
| Large Language Model Distilling Medication Recommendation Model                            | 已投稿 | TOIS       | 2024 | Qidong Liu, Xiangyu Zhao, Yuanshao Zhu, Zijian Zhang      | Xian Wu, Yefeng Zheng                           |
| LLM-ESR: Large Language Model Enhancement for Long-tail Sequential Recommendation          | 已录用 | AAAI       | 2025 | Qidong Liu*, Yejing Wang, Zijian Zhang                    | Xian Wu, Yefeng Zheng                           |
| Model Merging for Knowledge Editing in Medical Large Language Models                       | 已投稿 | ACL        | 2025 | 傅子川, 王叶晶, 赵翔宇                                             | kevinxwu (吴贤)                                   |

注：标有 * 的为主要作者/第一作者