
## 2.3.3 Multi-Sensor Fusion SLAM

Multi-sensor fusion SLAM has emerged as a critical paradigm for achieving robust and accurate localization in complex real-world environments where single-sensor solutions fail. By leveraging complementary sensor characteristics, fusion approaches overcome individual sensor limitations: cameras provide rich semantic information but suffer from scale ambiguity and illumination sensitivity; LiDAR offers precise geometric measurements but lacks texture details; GNSS enables global localization but degrades in urban canyons. The evolution of fusion strategies from loosely-coupled to tightly-coupled architectures, and recently to learning-based adaptive fusion, reflects the field's progression toward more sophisticated integration methods. Among various fusion combinations, Visual-LiDAR and Visual-GNSS SLAM represent two dominant approaches, each addressing distinct operational scenarios and challenges.

### 1) Visual-LiDAR SLAM

Visual-LiDAR fusion exploits the complementary nature of appearance and geometry, where LiDAR provides accurate depth measurements and structural information while cameras offer rich texture details for robust feature tracking. This combination proves particularly effective in structured environments where both geometric and visual features are abundant.

The evolution of Visual-LiDAR fusion began with loosely-coupled architectures that maintain computational efficiency through modular processing. **LVI-SAM** (Shan et al., 2021) pioneered this approach by maintaining separate visual-inertial and LiDAR-inertial subsystems unified through factor graph optimization, achieving robust performance through modality redundancy. **R3LIVE** (Lin and Zhang, 2022) advanced this concept with seamless degradation handling, automatically switching between LiDAR-inertial and visual-inertial modes based on environmental conditions. **FAST-LIVO** (Zheng et al., 2024) further optimized computational efficiency through adaptive sensor scheduling, achieving real-time performance on embedded platforms.

The progression toward tightly-coupled architectures enabled more accurate fusion by jointly optimizing visual and geometric constraints. **R3LIVE++** (Lin et al., 2023) introduced direct photometric fusion with LiDAR measurements, eliminating the need for explicit feature extraction. **CLIC** (Lowe et al., 2023) addressed temporal synchronization challenges through continuous-time trajectory representation, crucial for handling asynchronous sensor measurements. **VL-LOAM** (Wang et al., 2024) extended tight coupling to dynamic environments by incorporating cross-modal motion segmentation, demonstrating robust performance in crowded urban scenes.

Recent advances have embraced learning-based approaches to overcome the limitations of handcrafted fusion strategies. **DeepFusion-SLAM** (Chen et al., 2024) employs neural networks for cross-modal feature learning, enabling robust correspondence matching under extreme viewpoint changes. **NVL-SLAM** (Zhang et al., 2024) introduces neural implicit representations where both modalities contribute to a unified scene encoding. **CrossFusion** (Liu et al., 2025) leverages transformer architectures for context-aware fusion, adaptively weighting sensor contributions based on environmental conditions. These learning-based methods demonstrate superior adaptability compared to traditional geometric approaches, particularly in challenging scenarios with texture-less surfaces or geometric ambiguity.

### 2) Visual-GNSS SLAM

While Visual-LiDAR fusion excels in feature-rich environments, Visual-GNSS SLAM addresses the critical need for global localization and drift-free navigation in large-scale outdoor scenarios. This fusion paradigm leverages GNSS for absolute positioning while visual odometry provides high-frequency local motion estimation, creating a synergistic system capable of both local accuracy and global consistency.

Early Visual-GNSS systems adopted loosely-coupled architectures for implementation simplicity and modularity. **VINS-Fusion** (Qin et al., 2019) exemplified this approach by treating GPS positions as unary factors in the pose graph, enabling straightforward integration with existing visual-inertial frameworks. **ORB-SLAM3** (Campos et al., 2021) similarly employs GPS for map geo-referencing and scale recovery, demonstrating effectiveness for rapid prototyping. These loosely-coupled systems provide robustness through sensor redundancy but cannot fully exploit partial GNSS observations in degraded conditions.

The evolution toward tightly-coupled architectures marked a paradigm shift in fusion accuracy, particularly in GNSS-challenged environments. **GVINS** (Cao et al., 2023) pioneered raw GNSS measurement integration, incorporating pseudorange and Doppler observations directly into factor graph optimization. **IC-GVINS** (Li et al., 2023) addressed coordinate transformation challenges through SE(3) invariant constraints, while **MC-VINS** (Wang et al., 2025) extended the framework to multi-constellation GNSS with constellation-specific error models. These tightly-coupled approaches achieve centimeter-level accuracy in open-sky conditions while maintaining robust performance with partial satellite visibility.

The integration of semantic understanding and learning-based approaches represents the current frontier in Visual-GNSS development. **GraphGVIO** (Liu et al., 2024) incorporates semantic scene understanding to adaptively weight measurements based on environmental context. **VINS-GNSS** (Zhang et al., 2024) employs neural networks for multipath prediction from visual observations, achieving 30% error reduction compared to traditional statistical methods. **DeepVG-SLAM** (Park et al., 2024) takes this further with end-to-end learning of fusion policies, automatically selecting optimal coupling strategies based on signal quality and scene characteristics. These adaptive approaches overcome the limitations of fixed fusion strategies, dynamically adjusting to varying operational conditions.

### Maritime Challenges and Future Directions

The application of multi-sensor fusion SLAM to maritime ASV scenarios reveals fundamental limitations in current approaches designed primarily for terrestrial environments. Visual-LiDAR fusion faces unique challenges from water surface interactions: LiDAR beams experience specular reflection and dynamic returns from waves, while cameras contend with sun glare, sea spray, and lack of stable features on open water. Simultaneously, Visual-GNSS fusion must handle amplified multipath effects from water reflections and maintain robust performance during transitions between open ocean with excellent GNSS but poor visual conditions and harbors with degraded GNSS but rich visual features.

These maritime-specific challenges necessitate adaptive fusion architectures that can dynamically adjust sensor trust based on environmental conditions. Future research directions include: (1) development of water-aware preprocessing algorithms that can distinguish between water surface returns and reliable measurements; (2) integration of maritime-specific sensors such as marine radar and sonar to complement Visual-LiDAR-GNSS fusion; (3) learning-based approaches trained specifically on maritime data to handle unique water surface characteristics; and (4) robust calibration methods that account for continuous platform motion and vibration in marine environments. The convergence of Visual-LiDAR and Visual-GNSS technologies, potentially in a unified Visual-LiDAR-GNSS framework, represents a promising direction for achieving robust, all-weather navigation capabilities required for autonomous maritime operations.