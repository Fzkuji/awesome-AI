


```python
def softmax_with_eps(self, x):
    x = np.maximum(x, -1e10)  # 限制最小值
    exp_x = np.exp(x)
    return exp_x / (np.sum(exp_x) + self.eps)
```


哦，我理解了！你想要的是：**如果原始输入x小于0，则变成-inf；否则保持原值**。这相当于ReLU的变体。

## 正确的数学公式


$$\text{output}_i = \frac{\exp(\text{ReLU}_{\infty}(x_i))}{\sum_{j} \exp(\text{ReLU}_{\infty}(x_j)) + \epsilon}$$

其中 $$\text{ReLU}_{\infty}(x) = \begin{cases} x & \text{if } x \geq 0 \\ -\infty & \text{if } x < 0 \end{cases}$$

## 代码实现

```python
import numpy as np
import torch

def relu_inf_softmax(x, eps=1e-10):
    """
    将负值变为-inf，非负值保持不变，然后应用带epsilon的softmax
    """
    # Step 1: 负值变-inf，非负值保持
    masked_x = np.where(x >= 0, x, -np.inf)
    
    # Step 2: 带epsilon的softmax
    exp_x = np.exp(masked_x)
    return exp_x / (np.sum(exp_x) + eps)

# PyTorch版本
def relu_inf_softmax_torch(x, eps=1e-10):
    # 负值变-inf
    masked_x = torch.where(x >= 0, x, float('-inf'))
    
    # Softmax with epsilon
    exp_x = torch.exp(masked_x)
    return exp_x / (torch.sum(exp_x) + eps)

# 数值稳定版本
def stable_relu_inf_softmax(x, eps=1e-10):
    """
    数值稳定版本，处理全负值情况
    """
    mask = x >= 0
    
    # 检查是否全部为负值
    if not np.any(mask):
        # 全负值时返回均匀分布（或全零，取决于需求）
        return np.ones_like(x) / len(x)
    
    # 负值变-inf
    masked_x = np.where(mask, x, -np.inf)
    
    # 数值稳定的softmax（减去最大值）
    max_val = np.max(masked_x[mask])
    exp_x = np.exp(masked_x - max_val)
    
    return exp_x / (np.sum(exp_x) + eps)
```

## 示例

```python
x = np.array([-2.0, 3.0, -1.0, 5.0, 0.0, 2.0])

result = relu_inf_softmax(x)
print(f"输入: {x}")
print(f"掩码后: {np.where(x >= 0, x, -np.inf)}")
print(f"输出: {result}")
print(f"和: {np.sum(result)}")

# 输入: [-2.  3. -1.  5.  0.  2.]
# 掩码后: [-inf   3. -inf   5.   0.   2
```